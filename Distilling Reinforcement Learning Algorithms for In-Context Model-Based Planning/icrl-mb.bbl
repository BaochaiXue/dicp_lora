\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anand et~al.(2022)Anand, Walker, Li, V{\'{e}}rtes, Schrittwieser, Ozair, Weber, and Hamrick]{ProcGenMuZero}
Ankesh Anand, Jacob~C. Walker, Yazhe Li, Eszter V{\'{e}}rtes, Julian Schrittwieser, Sherjil Ozair, Theophane Weber, and Jessica~B. Hamrick.
\newblock Procedural generalization by planning with self-supervised world models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang]{GPT4}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John~A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuan-Fang Li, Scott~M. Lundberg, Harsha Nori, Hamid Palangi, Marco~Tulio Ribeiro, and Yi~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{DT}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Clavera et~al.(2018)Clavera, Rothfuss, Schulman, Fujita, Asfour, and Abbeel]{MB-MPO}
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and P.~Abbeel.
\newblock Model-based reinforcement learning via meta-policy optimization.
\newblock In \emph{Conference on Robot Learning}, 2018.

\bibitem[Dorfman et~al.(2021)Dorfman, Shenfeld, and Tamar]{BOReL}
Ron Dorfman, Idan Shenfeld, and Aviv Tamar.
\newblock Offline meta reinforcement learning - identifiability challenges and effective data collection strategies.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and Abbeel]{RL2}
Yan Duan, John Schulman, Xi~Chen, Peter~L. Bartlett, Ilya Sutskever, and P.~Abbeel.
\newblock {RL\textsuperscript{2}: Fast Reinforcement Learning via Slow Reinforcement Learning}.
\newblock \emph{ArXiv preprint}, 2016.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{MAML}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Ha \& Schmidhuber(2018)Ha and Schmidhuber]{WM}
David Ha and J{\"{u}}rgen Schmidhuber.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{SAC}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and Lillicrap]{DreamerV3}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy~P. Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Hiraoka et~al.(2021)Hiraoka, Imagawa, Tangkaratt, Osa, Onishi, and Tsuruoka]{M3PO}
Takuya Hiraoka, Takahisa Imagawa, Voot Tangkaratt, Takayuki Osa, Takashi Onishi, and Yoshimasa Tsuruoka.
\newblock Meta-model-based meta-policy optimization.
\newblock In \emph{Asian Conference on Machine Learning}, 2021.

\bibitem[Huang et~al.(2024)Huang, Hu, Chen, Sun, and Yang]{IDT}
Sili Huang, Jifeng Hu, Hechang Chen, Lichao Sun, and Bo~Yang.
\newblock In-context decision transformer: Reinforcement learning via hierarchical chain-of-thought.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{MBPO}
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{TTO}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Milos, Osinski, Campbell, Czechowski, Erhan, Finn, Kozakowski, Levine, Mohiuddin, Sepassi, Tucker, and Michalewski]{SimPLe}
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy~H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski.
\newblock Model based reinforcement learning for atari.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Laskin et~al.(2023)Laskin, Wang, Oh, Parisotto, Spencer, Steigerwald, Strouse, Hansen, Filos, Brooks, Gazeau, Sahni, Singh, and Mnih]{AD}
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ~Strouse, Steven~Stenberg Hansen, Angelos Filos, Ethan~A. Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih.
\newblock In-context reinforcement learning with algorithm distillation.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Lee et~al.(2023{\natexlab{a}})Lee, Xie, Pacchiano, Chandak, Finn, Nachum, and Brunskill]{DPT}
Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill.
\newblock Supervised pretraining can learn in-context reinforcement learning.
\newblock In \emph{Neural Information Processing Systems}, 2023{\natexlab{a}}.

\bibitem[Lee et~al.(2022)Lee, Nachum, Yang, Lee, Freeman, Xu, Guadarrama, Fischer, Jang, Michalewski, and Mordatch]{MGDT}
Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, L.~Y. Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian~S. Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch.
\newblock Multi-game decision transformers.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Lee et~al.(2023{\natexlab{b}})Lee, Son, and Kim]{MCL-TF}
Soochan Lee, Jaehyeon Son, and Gunhee Kim.
\newblock Recasting continual learning as sequence modeling.
\newblock In \emph{Neural Information Processing Systems}, 2023{\natexlab{b}}.

\bibitem[Lee et~al.(2024)Lee, Jeon, Son, and Kim]{MCL-SB}
Soochan Lee, Hyeonseong Jeon, Jaehyeon Son, and Gunhee Kim.
\newblock Learning to continually learn with the bayesian principle.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{offlineRL}
Sergey Levine, Aviral Kumar, G.~Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock \emph{ArXiv preprint}, 2020.

\bibitem[Li et~al.(2021)Li, Yang, and Luo]{FOCAL}
Lanqing Li, Rui Yang, and Dijun Luo.
\newblock {FOCAL:} efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lin et~al.(2020)Lin, Thomas, Yang, and Ma]{AdMRL}
Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma.
\newblock Model-based adversarial meta-reinforcement learning.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Liu \& Abbeel(2023)Liu and Abbeel]{AT}
Hao Liu and P.~Abbeel.
\newblock Emergent agentic transformer from chain of hindsight experience.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Mitchell et~al.(2021)Mitchell, Rafailov, Peng, Levine, and Finn]{MACAW}
Eric Mitchell, Rafael Rafailov, Xue~Bin Peng, Sergey Levine, and Chelsea Finn.
\newblock Offline meta-reinforcement learning with advantage weighting.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Kahn, Fearing, and Levine]{MB-MPC}
Anusha Nagabandi, Gregory Kahn, Ronald~S. Fearing, and Sergey Levine.
\newblock Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning.
\newblock \emph{IEEE International Conference on Robotics and Automation}, 2018.

\bibitem[Nagabandi et~al.(2019)Nagabandi, Clavera, Liu, Fearing, Abbeel, Levine, and Finn]{ReBAL}
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald~S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
\newblock Learning to adapt in dynamic, real-world environments through meta-reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{Reptile}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{ArXiv preprint}, 2018.

\bibitem[Pinon et~al.(2022)Pinon, Delvenne, and Jungers]{TFsearch}
Brieuc Pinon, Jean-Charles Delvenne, and Rapha{\"e}l~M. Jungers.
\newblock A model-based approach to meta-reinforcement learning: Transformers and tree search.
\newblock \emph{European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning}, 2022.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann]{SB3}
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and Quillen]{PEARL}
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic context variables.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi, Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{GATO}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost~Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley~D. Edwards, Nicolas Manfred~Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de~Freitas.
\newblock A generalist agent.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Rimon et~al.(2022)Rimon, Tamar, and Adler]{densityEst}
Zohar Rimon, Aviv Tamar, and Gilad Adler.
\newblock Meta reinforcement learning with finite training tasks - a density estimation approach.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Rimon et~al.(2024)Rimon, Jurgenson, Krupnik, Adler, and Tamar]{MAMBA}
Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, and Aviv Tamar.
\newblock Mamba: an effective world model approach for meta-reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[S{\ae}mundsson et~al.(2018)S{\ae}mundsson, Hofmann, and Deisenroth]{GP}
Steind{\'{o}}r S{\ae}mundsson, Katja Hofmann, and Marc~Peter Deisenroth.
\newblock Meta reinforcement learning with latent variable gaussian processes.
\newblock In \emph{Conference on Uncertainty in Artificial}, 2018.

\bibitem[Schrittwieser et~al.(2019)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and Silver]{MuZero}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, L.~Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy~P. Lillicrap, and David Silver.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{Nature}, 2019.

\bibitem[Schrittwieser et~al.(2021)Schrittwieser, Hubert, Mandhane, Barekatain, Antonoglou, and Silver]{Reanalyse}
Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver.
\newblock Online and offline reinforcement learning by planning with a learned model.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and Moritz]{TRPO}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I. Jordan, and Philipp Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{PPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv preprint}, 2017.

\bibitem[Sinii et~al.(2024)Sinii, Nikulin, Kurenkov, Zisman, and Kolesnikov]{Headless-AD}
Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, and Sergey Kolesnikov.
\newblock In-context reinforcement learning for variable action spaces.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Son et~al.(2024)Son, Lee, and Kim]{MCL-Survey}
Jaehyeon Son, Soochan Lee, and Gunhee Kim.
\newblock When meta-learning meets online and continual learning: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{TF}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2016)Wang, Kurth-Nelson, Soyer, Leibo, Tirumala, Munos, Blundell, Kumaran, and Botvinick]{LtoRL}
Jane~X. Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel~Z. Leibo, Dhruva Tirumala, R{\'e}mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew~M. Botvinick.
\newblock Learning to reinforcement learn.
\newblock \emph{ArXiv preprint}, 2016.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Zhang, Jiang, Zhang, Wang, and Zhang]{IDAQ}
Jianhao Wang, Jin Zhang, Haozhe Jiang, Junyu Zhang, Liwei Wang, and Chongjie Zhang.
\newblock Offline meta reinforcement learning with in-distribution online adaptation.
\newblock In \emph{International Conference on Machine Learning}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Bing, Yao, Wang, Su, Yang, Huang, and Knoll]{MoSS}
Mingyang Wang, Zhenshan Bing, Xiangtong Yao, Shuai Wang, Hang Su, Chenguang Yang, Kai Huang, and Alois Knoll.
\newblock Meta-reinforcement learning based on self-supervised task representation learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, hsin Chi, Xia, Le, and Zhou]{COT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Huai hsin Chi, F.~Xia, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Ye et~al.(2021)Ye, Liu, Kurutach, Abbeel, and Gao]{EfficientZero}
Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao.
\newblock Mastering atari games with limited data.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and Levine]{MW}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan~C. Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, 2019.

\bibitem[Yuan \& Lu(2022)Yuan and Lu]{CORRO}
Haoqi Yuan and Zongqing Lu.
\newblock Robust task representations for offline meta-reinforcement learning via contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu]{TinyLlama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{ArXiv preprint}, 2024.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann, and Whiteson]{VariBAD}
Luisa~M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson.
\newblock Varibad: {A} very good method for bayes-adaptive deep {RL} via meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zintgraf et~al.(2021)Zintgraf, Feng, Lu, Igl, Hartikainen, Hofmann, and Whiteson]{HyperX}
Luisa~M. Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon Whiteson.
\newblock Exploration in approximate hyper-state space for meta reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Zisman et~al.(2024)Zisman, Kurenkov, Nikulin, Sinii, and Kolesnikov]{AD-eps}
Ilya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viacheslav Sinii, and Sergey Kolesnikov.
\newblock Emergence of in-context reinforcement learning from noise distillation.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\end{thebibliography}
