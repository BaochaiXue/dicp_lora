
\section{Related Work}
Creating capable robots without requiring explicit programming of behaviors is a longstanding challenge in the field \cite{atkeson1997robot, argall2009survey, ravichandar2020recent}.
While conceptually simple, behavior cloning has shown surprising promise on an array of real-world robot tasks, including manipulation \cite{zhang2018deep, florence2019self, mandlekar2020learning, mandlekar2020iris, zeng2021transporter, rahmatizadeh2018vision, avigal2022speedfolding} and autonomous driving \cite{pomerleau1988alvinn, bojarski2016end}. Current behavior cloning approaches can be categorized into two groups, depending on the policy's structure.


\textbf{Explicit Policy.}
The simplest form of explicit policies maps from world state or observation directly to action \cite{pomerleau1988alvinn, zhang2018deep, florence2019self, ross2011reduction, toyer2020magical, rahmatizadeh2018vision, bojarski2016end}. They can be supervised with a direct regression loss and have efficient inference time with one forward pass. Unfortunately, this type of policy is not suitable for modeling multi-modal demonstrated behavior,
and struggles with high-precision tasks \cite{ibc}.
A popular approach to model multimodal action distributions while maintaining the simplicity of direction action mapping is convert the regression task into classification by discretizing the action space \cite{zeng2021transporter, wu2020spatial, avigal2022speedfolding}.
However, the number of bins needed to approximate a continuous action space grows exponentially with increasing dimensionality.
Another approach is to combine Categorical and Gaussian distributions to represent continuous multimodal distributions via the use of MDNs \cite{bishop1994mixture, robomimic} or clustering with offset prediction \cite{bet, sharma2018multiple}. Nevertheless, these models tend to be sensitive to hyperparameter tuning, exhibit mode collapse, and are still limited in their ability to express high-precision behavior \cite{ibc}.



\textbf{Implicit Policy.}
Implicit policies \citep{ibc, jarrett2020strictly} define distributions over actions by using Energy-Based Models (EBMs) \citep{lecun06atutorial, du2019implicit, dai2019exponential, grathwohl2020stein, du2020improved}.
In this setting, each action is assigned an energy value, with action prediction corresponding to the optimization problem of finding a minimal energy action. Since different actions may be assigned low energies, implicit policies naturally represent multi-modal distributions. However, existing implicit policies \citep{ibc} are unstable to train due to the necessity of drawing negative samples when computing the underlying Info-NCE loss. %







\textbf{Diffusion Models.}
Diffusion models are probabilistic generative models that iteratively refine randomly sampled noise into draws from an underlying distribution. They can also be conceptually understood as learning the gradient field of an implicit action score and then optimizing that gradient during inference.
Diffusion models \citep{sohldickstein2015nonequilibrium, ho2020denoising} have recently been applied to solve various different control tasks \citep{janner2022diffuser, urain2022se, ajay2022conditional}.

In particular, \citet{janner2022diffuser} and \citet{huang2023diffusion} explore how diffusion models may be  used in the context of planning and infer a trajectory of actions that may be executed in a given environment.
In the context of Reinforcement Learning, \citet{wang2022diffusion} use diffusion model for policy representation and regularization with state-based observations.
In contrast, in this work, we explore how diffusion models may instead be effectively applied in the context of behavioral cloning for effective visuomotor control policy.
To construct effective visuomotor control policies, we propose to combine DDPM's ability to predict high-dimensional action squences with closed-loop control, as well as a new transformer architecture for action diffusion and a manner to integrate visual inputs into the action diffusion model.

\citet{wang2023diffusion} explore how diffusion models learned from expert demonstrations can be used to augment classical explicit polices without directly taking advantage of diffusion models as policy representation.

Concurrent to us, \citet{pearce2023imitating}, \citet{reuss2023goal} and \citet{hansen2023idql} has conducted a complimentary analysis of diffusion-based policies in simulated environments. While they focus more on effective sampling strategies, leveraging classifier-free guidance for goal-conditioning as well as applications in Reinforcement Learning, and we focus on effective action spaces, our empirical findings largely concur in the simulated regime.  In addition, our extensive real-world experiments provide strong evidence for the importance of a receding-horizon prediction scheme, the careful choice between velocity and position control, and the necessity of optimization for real-time inference and other critical design decisions for a physical robot system.
