

\section{Introduction}
\label{sec:intro}
Reinforcement learning (RL) holds the promise of solving any given task based only on a reward function. However, this simple and direct formulation of the RL problem is often impractical: in complex environments, exploring entirely from scratch to learn an effective policy can be prohibitively expensive, as it requires the agent to successfully solve the task through random chance before learning a good policy. Indeed, even humans and animals rarely solve new tasks entirely from scratch, instead leveraging prior knowledge and skills from past experience. Inspired by this, a number of recent works have sought to incorporate prior offline data into online RL exploration~\citep{hu2023unsupervised, li2024accelerating, wilcoxson2024leveraging}.
But this poses a new set of challenges: the distribution of offline data might not match the policy that the agent should follow online, introducing distributional shift, and it is not obvious how the offline data should be leveraged to acquire a good online \emph{exploratory} policy.

In the adjacent field of imitation learning (IL), a widely used approach in recent years has been to employ \emph{action chunking}, where instead of training policies to predict a single action based on the state observation from prior data, the policy is instead trained to predict a short sequence of future actions (an ``action chunk'')~\citep{zhao2023learning, chi2023diffusion}. While a complete explanation for the effectiveness of action chunking in IL remains an open question, its effectiveness can be at least partially ascribed to better handling of non-Markovian behavior in the offline data, essentially providing a more powerful tool for modeling the kinds of complex distributions that might occur in (for example) human-provided demonstrations or mixtures of different behaviors~\citep{zhao2023learning}. Action chunking has not been used widely in RL, perhaps because optimal policies in fully observed MDPs are Markovian~\citep{sutton1998reinforcement}, and therefore chunking may appear unnecessary. 

We make the observation that, though we might desire a final optimal Markovian policy, the exploration problem can be better tackled with non-Markovian and temporally extended skills, and that action chunking offers a very simple and convenient recipe for obtaining this. Furthermore, action chunking provides a better way to leverage offline data (with a better handling of non-Markovian behavior in the data), and even improves the stability and efficiency of TD-based RL, by enabling unbiased $n$-step backups (where $n$ matches the length of the chunk). Thus, in combination with pretraining on offline data, action chunking offers a compelling and very simple way to mitigate the exploration challenge in RL. 

We present Q-learning with action chunking (or \hourpurple{Q-chunking} in short), a recipe for improving generic TD-based actor-critic RL algorithms in the offline-to-online RL setting (\cref{fig:banner}). The key idea is to run RL at an action sequence level --- (1) the policy predicts a sequence of actions for the next $h$ steps and executes them one-by-one open loop, and (2) the critic takes in the current state and a sequence of actions and estimates the value of carrying out the whole sequence rather than a single action. The benefits of operating RL on this extended action space are two-fold: (1) the policy can be optimized to generate temporally coherent actions by regularizing it towards some prior behavior data that exhibit such coherency, (2) the critic trained with a standard TD-backup loss is effectively performing $n$-step backups, with no off-policy bias (that typically occurs in na\"ive $n$-step return methods), since the critic takes the full action sequence into account. 

Our main contribution is \hourpurple{QC}, a practical offline-to-online RL algorithm that is instantiated from our \hourpurple{Q-chunking} recipe, essentially running RL with behavior regularization in the chunked action space. QC is simple to implement, requiring only training (1) an action chunking behavior policy using a standard flow-matching loss, and (2) a temporally extended critic with the standard TD-loss (\cref{algo:qc}). QC achieves strong performance on a range of six challenging long-horizon, sparse-reward domains, outperforming prior offline-to-online methods. Moreover, Q-chunking is a generic recipe that can be applied to existing offline-to-online algorithms with minimal modification. In this work, we demonstrate one such instantiation by applying it to \hourblue{FQL}~\citep{park2025flow}, resulting in \hourpurple{QC-FQL} (\cref{algo:qc-fql}), which shows significant improvements over the original method.





































\section{Related Work}
\label{sec:related}


\textbf{Offline-to-online reinforcement learning} methods focus on leveraging prior offline data to accelerate reinforcement learning online~\citep{xie2021policy, song2022hybrid, lee2022offline, NEURIPS2022_ba1c5356, zhang2023policy, zheng2023adaptive, ball2023efficient, nakamoto2024cal, zhou2024efficient, li2024accelerating}. The simplest way to tackle offline-to-online RL is to use an existing offline RL algorithm to first pretrain on the offline data and then use the same offline optimization objective to continue training online using a growing dataset that combines the original offline data and the replay buffer data~\citep{nair2020awac, kumar2020conservative, kostrikov2021offline, tarasov2024revisiting, park2025flow, agarwal2022reincarnating, luo2023finetuning, lee2022offline}. While straightforward, this na\"ive approach often result in overly pessimistic that hinders exploration and consequently the online sample-efficiency. Several prior works have attempted to address this issue by adjusting the degree of pessimism online~\citep{zhou2024efficient, nakamoto2024cal, luo2023finetuning, lee2022offline, wang2023train}. 
However, these approaches can be difficult to tune and sometimes stills fall short in online sample efficiency compared to a simple, well-regularized online RL algorithm learning from scratch on both offline data and online replay buffer data~\citep{ball2023efficient}.
Our approach takes a step towards improving the sample efficiency of offline-to-online RL methods via value backup acceleration and temporally coherent exploration.

\textbf{Action chunking} is a technique popularized by roboticists for imitation learning (IL), where the policy predicts and executes a sequence of actions in an  open-loop manner (``an action chunk'')~\citep{zhao2023learning}. Action chunking has been shown to improve policy robustness~\citep{zhao2023learning, george2023one, bharadhwaj2024roboagent}, and handle non-Markovian behavior in offline data~\citep{zhao2023learning}. 
Existing RL methods that incorporate action chunking typically focus on fine-tuning a policy pre-trained with imitation learning~\citep{ren2024diffusion, seo2024reinforcement}. \citet{tian2025chunking} propose to learn a critic on action chunks by integrating $n$-step returns with a transformer. However, their method only applies chunking to the critic, while still optimizing a single-step actor.

\textbf{Exploration with temporally coherent actions.} 
Existing methods either rely on temporally correlated action noises~\citep{lillicrap2015continuous} that are constructed through heuristics; hierarchically structured policies (see the next paragraph), which are often tricky to stabilize during online training; or pre-trained frozen skill policies~\citep{pertsch2021accelerating, wilcoxson2024leveraging}, which are not amendable for fine-grained online fine-tuning. Our method uses a single network to represent the policy to generate temporally extended action chunk and it is trained using a single objective function that is stable to optimize. There is also no frozen, pretrained components in our approach, ensuring its online fine-tuning flexibility. 


\textbf{Hierarchical reinforcement learning, options framework.} Learning temporally extended actions have also been widely studied in the hierarchical reinforcement learning (HRL) literature~\citep{dayan1992feudal, dietterich2000hierarchical, vezhnevets2016strategic, daniel2016hierarchical, kulkarni2016hierarchical, vezhnevets2017feudal, peng2017deeploco, riedmiller2018learning, nachum2018data, ajay2020opal, shankar2020learning, pertsch2021accelerating, gehring2021hierarchical, xie2021latent}. HRL methods typically train a space of low-level policies that can directly interact with the environment along with a high-level policy that selects among these low-level policies. These low-level policies can be hand-crafted~\citep{dalal2021accelerating}, automatically discovered online~\citep{dietterich2000hierarchical, kulkarni2016hierarchical, vezhnevets2016strategic, vezhnevets2017feudal, nachum2018data}, or pretrained using offline skill discovery methods~\citep{paraschos2013probabilistic,merel2018neural,shankar2020learning, ajay2020opal, singh2020parrot, pertsch2021accelerating, touati2022does, nasiriany2022learning, hu2023unsupervised, frans2024unsupervised, chen2024self, park2024foundation}. The options framework provides a slightly more sophisticated and more powerful formulation, where the low-level policy is additionally associated with learnable initiation condition and termination condition that makes utilization of the low-level policy more flexible~\citep{sutton1999between, menache2002q, chentanez2004intrinsically, mannor2004dynamic, csimcsek2004using, csimcsek2007betweenness, konidaris2011autonomous, daniel2016hierarchical, srinivas2016option, oh2017value, fox2017multi, bacon2017option, kim2019variational, bagaria2019option, bagaria2024effectively, de2025learning}.  A long-lasting challenge in HRL is its bi-level optimization problem: when both low-level and high-level policies are updated during training, the high-level policies must optimize a moving objective function, which can lead to instability~\citep{nachum2018data}. To mitigate this, some methods keep the low-level policies frozen after initial pretraining~\citep{ajay2020opal, pertsch2021accelerating, wilcoxson2024leveraging} to improve stability during online training. Our approach is a special case of HRL where the low-level skill executes a sequence of actions open-loop. This design choice allows us to collapse the bi-level optimization problem into a standard RL objective in a temporally extended action space, while retaining many of the exploration benefits associated with HRL methods.

\textbf{Multi-step latent space planning and search} is a technique commonly used in model-based RL methods where they use a learned model to optimize a short-horizon action sequence towards high-return trajectories~\citep{oh2017value, schrittwieser2020mastering}. 
These approaches work by training a dynamics model on an encoded latent space, where the model takes in a latent state and an action to predict the next latent state and the associated reward value. This latent dynamics model, along with a value network on the latent state, can then provide an estimate of the $Q$-value on-the-fly for any given action sequence starting from a given latent state by simply simulating the action sequence in the latent dynamics model. In contrast, we do not learn a latent dynamics model and instead train a $Q$-network to directly estimate the value of the action sequence.
Lastly, these approaches operate in the purely online RL setting whereas we focus on the offline-to-online RL setting.




\section{Background}
\label{sec:preliminary}
\textbf{Offline-to-online RL.} In this paper, we consider an infinite-horizon, fully observable Markov decision process (MDP), $(\mathcal{S}, \mathcal{A}, \rho, T, r, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $T(s'|s, a): \mathcal{S} \times \mathcal{A} \mapsto \Delta(\mathcal{S})$ is the transition kernel, $r(s, a): \mathcal{S}\times \mathcal{A}\mapsto \mathbb{R}$ is the reward function, $\rho: \Delta(\mathcal{S})$ is the initial state distribution and $\gamma \in [0, 1)$ is the discount factor. We also assume there is a prior offline dataset $\mathcal{D}$ that consists of transitions rollouts $\{(s, a, s', r)\}$ from $\mathcal{M}$. The goal of offline-to-online RL is to find a policy $\pi(a | s): \mathcal{S} \mapsto \Delta(\mathcal{A})$ that maximizes the expected discounted cumulative reward (or discounted return): $\eta(\pi) := \mathbb{E}_{s_{t+1} \sim T(s_t, a_t), a_t \sim \pi(\cdot | s_{t})}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right]$.
Oftentimes, offline-to-online RL algorithms operate in two distinct phases: an offline phase where a policy is pretrained on the offline data $\mathcal{D}$ and an online phase where the policy is further fine-tuned online with environment interactions. Our approach follows the same regime.




\textbf{Temporal difference and multi-step return.} TD-based RL algorithms typically learn $Q_\theta(s, a)$ to approximate the maximum expected discounted cumulative reward that a policy can receive starting from state $s$ and action $a$ by using a temporal difference (TD) loss~\citep{sutton1998reinforcement}:
\begin{align}
    L(\theta) = \left[Q_\theta(s_t, a_t) - \hat{V}\right]^2,
\end{align}
where $\hat{V}$ is an estimate of $Q(s_t, a_t)$ that is commonly chosen as $\hat{V}_{\mathrm{1\text{-}step}}$:
\begin{align}
    \hat{V}_{\mathrm{1\text{-}step}} := r_t + \gamma Q_{\bar \theta}(s_{t+1}, a_{t+1}), \quad a_{t+1} \sim \pi_\psi(\cdot| s_{t+1}),
\end{align}
and $s_t, a_t, s_{t+1}, r$ are sampled from some off-policy trajectories and $\bar \theta$ is a delayed version of $\theta$ that does not allow the gradient to pass through for learning stability. When the TD error is minimized, the $Q_\theta$ converges to the expected discounted value of the policy $\pi_\psi$. As the effective horizon $\tilde{H} = 1 / (1-\gamma)$ goes up, the learning slows down as the value only propagates 1 step backward (from $s_{t+1}$ to $s_t$). To speed-up long-horizon value backup, a common strategy is to sample a length-$n$ trajectory segment, ($s_t, a_t, s_{t+1}, \cdots, a_{t+n-1}, s_{t+n})$, and construct a $n$-step return from it~\citep{watkins1989learning, sutton1998reinforcement}:
\begin{align}
    \hat{V}_{\mathrm{n\text{-}step}} := \sum_{t'=t}^{t+n-1} \left[\gamma^{t'-t}r_t'\right] + Q_{\bar \theta}(s_{t+n}, a_{t+n}), \quad a_{t+n} \sim \pi_\psi(\cdot | s_{t+n}),
    \label{eq:nstep}
\end{align}
where again $r_t = r(s_t, a_t)$. This value estimate of $Q(s_t, a_t)$ allows for a $n$ times speed-up in terms of the number of time steps that the value can propagate back across. This estimator is sometimes referred to as the \emph{uncorrected $n$-step return estimator}~\citep{fedus2020revisiting, kozuno2021revisiting} because it is biased when the data collection policy is different from the current policy $\pi_\psi$. 
Nevertheless, due to the implementation simplicity of $n$-step return, it has been commonly adopted in large-scale RL systems~\citep{mnih2016asynchronous, hessel2018rainbow, kapturowski2018recurrent, wurman2022outracing}. 



\section{\ourslong{}}
\label{sec:method}
In this section, we first describe two main design principles of \hourpurple{Q-chunking}: (1) Q-learning on a temporally extended action space (the space of chunks of actions), and (2) behavior constraint in this extended action space, followed by practical implementations of Q-chunking (\hourpurple{QC}, \hourpurple{QC-FQL}) as effective TD-based offline-to-online RL algorithms.

\subsection{Q-learning on a temporally extended action space} 
\label{sec:qc-v}
The first design principle of Q-chunking is to apply $Q$-learning on the temporally extended action space. Unlike normal 1-step TD-based actor-critic methods, which train a Q-function $Q(s_t, a_t)$ and a policy $\pi(a_t | s_t)$, we instead train both the critic and the actor with a span of $h$ consecutive actions:~\footnote{We use $\ac{t}{t+h}$ to denote a concatenation of $h$ consecutive actions: $\begin{bmatrix} a_t & a_{t+1} & \cdots & a_{t+h-1} \end{bmatrix} \in \mathbb{R}^{Ah}$ for notation convenience. This is similar for $\ssc{t}{t+h}$ and $\rrc{t}{t+h}$.}
\begin{align*}
 &\text{\emph{Q-Chunking Policy: }} \pi_\psi({\color{ourpurple} \ac{t}{t+h}} | s_t) :=\pi_\psi( {\color{ourpurple} a_t, a_{t+1}, \cdots, a_{t+h-1}} | s_t) \\
 &\text{\emph{Q-Chunking Critic: }} Q_\theta(s_t, {\color{ourpurple} \ac{t}{t+h}}) := Q_\theta(s_t, {\color{ourpurple} a_t, a_{t+1}, \cdots, a_{t+h-1}})  
\end{align*}
In practice, this involves updating the critic and the actor on batches of transitions consisting of a random state $s_t$, an action sequence followed by the state $\mathbf a_t$, and the state $h$ steps into the future, $s_{t+h}$. Specifically, we train $Q_\theta$ with the following TD loss,
\begin{align}
    L(\theta) = \mathbb{E}_{s_t, {\color{ourpurple}\ac{t}{t+h}, s_{t+h}} \sim \mathcal{D}}
    \left[\left(Q_\theta(s_t, {\color{ourpurple}\ac{t}{t+h}}) - {\color{ourpurple}\sum_{t'=1}^{h} \gamma^{t'}r_{t+t'}} - 
    \gamma^{\color{ourpurple}h} Q_{\bar \theta}({\color{ourpurple}s_{t+h}},  {\color{ourpurple} \ac{t+h}{t+2h}})\right)^2\right]
\end{align}
with ${\color{ourpurple} \ac{t+h}{t+2h}} \sim \pi_\psi(\cdot | s_{t+h})$, and $\bar \theta$ being the target network parameters that are often an exponential moving average of $\theta$~\citep{haarnoja2018soft}.

The TD loss above shares striking similarity to the $n$-step return in Equation \ref{eq:nstep} (with $n$ matches $h$) but with a crucial difference --- the $Q$-function used in the $n$-step return backup takes in only one action (at time step $t$) whereas our $Q$-function takes in the whole action sequence. The implication of this difference can be best explained after we write out the TD backup equations for standard 1-step TD, $n$-step return, and Q-chunking:
\begin{align}
    Q(s_t, a_t) & \leftarrow r_t + \gamma Q(s_{t+1}, a_{t+1})  & \text{(standard 1-step TD)} \\
    Q(s_t, a_t) &\leftarrow \underbrace{\sum_{t'=t}^{t+h-1} \left[\gamma^{t'-t}r_{t'}\right]}_\text{\color{ourblue} biased} + \gamma^hQ(s_{t+h}, a_{t+h}), & \text{\color{ourblue}($n$-step return, $n=h$)} \\
    Q(s_t, {\color{ourpurple}\ac{t}{t+h}}) &\leftarrow \underbrace{\sum_{t'=t}^{t+h-1} \left[\gamma^{t'-t}r_{t'}\right]}_\text{\color{ourpurple} unbiased} + \gamma^h Q(s_{t+h}, {\color{ourpurple}\ac{t+h}{t+2h}}). & \text{\hourpurple{(Q-chunking)}}
\end{align}
For the standard 1-step TD, each backup step propagates the value back by only 1 time step. $n$-step return propagates the value back $h \times$ faster, but can suffer from a \emph{biased} value estimation issue when $\ssc{t}{t+h}$ and $\ac{t}{t+h}$ are off-policy~\citep{fedus2020revisiting}. This is because the discounted sum of the $n$-step rewards $\rrc{t}{t+h}$ from the dataset or replay buffer is no longer an unbiased estimate of the expected $n$-step rewards under the current policy $\pi$. Q-chunking value backup is similar to the $n$-step return where each step also propagates the value back by $h$ time steps, but \emph{does not} suffer from this biased estimation issue. Unlike $n$-step return where we are propagating the value to a 1-step $Q$-function, Q-chunking backup propagates the value back to a $h$-step $Q$-function that takes in the exact same actions that are taken to obtain the $n$-step rewards $\rrc{t}{t+h}$, eliminating the biased value estimation. As a result, Q-chunking value backup enjoys the value propagation speedup while maintaining an unbiased value estimate. 


















    





\subsection{Behavior constraints for temporally coherent exploration}

\label{sec:qc-p}




The second design principle of Q-chunking addresses the action incoherency issues by leveraging a behavior constraint in the objective for the $\pi_\psi$:
\begin{align}
    L(\psi) = -\mathbb{E}_{s_t \sim \mathcal{D}, \ac{t}{t+h} \sim \pi_\psi(\cdot | s_t)}\left[Q_\theta(s_t, \ac{t}{t+h})\right], \text{s.t. }  {\transparent{0.15}\mathcolorbox{ourpurple}{\transparent{1.0}D(\pi_\psi(\ac{t}{t+h} | s_t), \pi_\beta(\ac{t}{t+h} | s_t)) \leq \varepsilon}}
    \label{eq:qc-p}
\end{align}
where we denote $\pi_\beta(\ac{t}{t+h} | s_t)$ as the behavior distribution in the offline data $\mathcal{D}$, and $D$ as some distance metric that measures how different the learned policy $\pi$ deviates from $\pi_\beta$. 

Intuitively, a behavior constraint on the temporally extended action sequence allows us to leverage temporally coherent action sequences in the offline dataset. This is particularly advantageous in the temporally extended action space compared to in the original action space because offline data often exhibit non-Markovian structure (e.g., from scripted policies~\citep{park2024ogbench}, human tele-operators~\citep{robomimic2021}, or noisy expert policies for sub-tasks~\citep{park2024ogbench, fu2020d4rl}) that cannot be well captured by a Markovian behavior constraint. 
Temporally coherent actions are desirable for online exploration because they resemble temporally extended skills (e.g., moving in a certain direction for navigation, jumping motions for going over obstacles) that help traverse the environment in a structured way rather than using random actions that often result in data that is localized near the initial states. 
Imposing behavior constraint for an action chunking policy is a very simple way to approximately extract skills without the need of training policy with bi-level structure as often necessitated by skill-based methods (see more discussion in Section \ref{sec:related}). In reality, we do see that Q-chunking, with such behavior constraints, can interact and explore the environment with temporally coherent actions (see Section \ref{sec:results-analysis}), 
mitigating the exploration challenge in RL.  
















\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/rlpd-qc-all.pdf}  %
\vspace{-0.7em}
\caption{\footnotesize \textbf{Na\"ively using action chunking for online RL with Gaussian policies leads to poor performance.} \emph{(1)} \hourblue{RLPD} runs online RL on both offline data and online replay buffer~\citep{ball2023efficient}. \emph{(2)} \hourblue{RLPD-AC} is the same algorithm as RLPD but operates in a temporally extended action space (action chunk size of 5). \emph{(3)} \hourpurple{QC-RLPD} additionally uses a behavior cloning loss on the actor (4 seeds). }
\vspace{-1em}
\label{fig:rlpd-all}
\end{figure}

\subsection{Practical implementations}
A key implementation challenge of Q-chunking is to enforce a good behavior constraint that captures the non-Markovian behavior at the action sequence level. One prerequisites of imposing a good behavior constraint is the ability of the policy to capture the complex behavior distribution (e.g., with a flow/diffusion policy). A Gaussian policy, a default choice in online RL algorithms, does not suffice. Indeed, if we na\"ively take an off-the-shelf online algorithm, \hourblue{RLPD}~\citep{ball2023efficient} for example, and apply Q-chunking with a behavior cloning loss, we find that it often performs poorly (\cref{fig:rlpd-all}). 

To enforce a good behavior constraint, we start by using flow-matching objective~\citep{liu2022flow} to train a behavior cloning flow policy to capture the behavior distribution. The flow policy is parameterized by a state-conditioned velocity field prediction model $f(s, \mathbf z, u): \mathcal{S} \times \mathbb{R}^{Ah} \times [0, 1] \mapsto \mathbb{R}^{Ah}$ and we denote $f_\xi(\cdot | s)$ as the action distribution that the flow policy parameterizes, which serves as an approximation of the true behavior distribution in the offline data ($f_\xi \approx \pi_\beta$). Now, we are ready to present our main method:

\hourpurple{QC}\textbf{: Q-chunking with implicit KL behavior constraint.}
We consider a KL constraint on our policy through the learned behavior distribution:
\begin{align}
    D_{\mathrm{KL}}(\pi_\psi \| f_\xi(\cdot | s)) \leq \varepsilon
    \label{eq:kl}
\end{align}
While it is possible include the KL as part of the loss, estimating the KL divergence or log probability for flow models is practically challenging. Instead, we use best-of-$N$ sampling~\citep{stiennon2020learning} to maximize $Q$-value while imposing this KL constraint implicitly altogether. Practically, this involves first sampling $N$ action chunks from the learned behavior policy $f_\xi(\cdot | s_t)$,
\begin{align*}
    \{\mathbf{a}^1, \mathbf{a}^2, \cdots, \mathbf{a}^N\} \sim f_\xi(\cdot | s),
\end{align*}
and then picking the action chunk sample that maximizes the temporally extended $Q$-function:
\begin{align*}
    \mathbf{a}^\star \leftarrow {\arg\max}_{\mathbf a \in \{\mathbf a^1, \mathbf a^2, \cdots, \mathbf a^N\}} Q(s, \mathbf{a})
\end{align*}
It has been shown in prior work that best-of-$N$ sampling admits a closed-form upper-bound on the KL divergence from the original distribution~\citep{hilton2023kl}:
\begin{align}
    D_{\mathrm{KL}}(\mathbf{a}^\star \| f_\xi(\cdot | s)) \leq \log N - \frac{N-1}{N},
\end{align}
which approximately satisfies KL constraint implicitly (Equation \ref{eq:kl}). Tuning the value of $N$ directly corresponds to the strength of the constraint.

Since we approximate the policy optimization (Equation \ref{eq:qc-p}) with the best-of-$N$ sampling, we can completely avoid separately parameterizing a policy $\pi_\psi$ and only sample from the behavioral policy $f_\xi(\cdot | s_t)$. In particular, we use the best-of-$N$ sampling to generate actions to both (1) interact with the environment, and (2) provide the action samples in the TD backup following \citet{ghasemipour2021emaq}. As a result, our algorithm has only one additional loss function:
\begin{align}
    L(\theta) = \mathbb{E}_{\substack{s_t, \mathbf{a}_t \sim D \\ \{\color{ourblue}{\mathbf{a}^{i}_{t+h}\}_{i=1}^{N} \sim f_\xi(\cdot|s_{t+h})}}} \left[\left(Q_\theta(s_t, \mathbf{a}_t) - \sum_{t'=1}^{h} \gamma^{t'}r_{t+t'} - \gamma^{h} Q_{\bar \theta}(s_{t+h}, {\color{ourpurple}\mathbf{a}^\star_{t+h}})\right)^2\right]
    \label{eq:qc-critic}
\end{align}
where again ${\color{ourpurple}\mathbf{a}^\star_{t+h} := {\arg\max}_{\mathbf a \in \{\mathbf a_{t+h}^{i}\}} Q(s, \mathbf{a})}$. 

While QC is simple and easy to implement, it does come with some additional computational costs (sampling $N\times$). 

We present a variant of our method below that leverages a cheaper off-the-shelf offline/offline-to-online RL method, \hourblue{FQL}~\citep{park2025flow}. 

\hourpurple{QC-FQL}\textbf{: Q-chunking with 2-Wasserstein distance behavior constraint.} For this variant of our method, we leverage the optimal transport framework to impose a Wasserstein distance ($W_2$) constraint, again, through the learned behavior policy $f_\xi(\cdot | s)$:
\begin{align}
    W_2 (\pi_\psi, f_\xi(\cdot | s)) \leq \varepsilon
\end{align}
Following \hourblue{FQL}~\citep{park2025flow}, we parameterize the policy $\pi_\psi$ with a noise-conditioned action prediction model, $\mu_\psi(s, \mathbf z): \mathcal{S} \times \mathbb{R}^{Ah} \mapsto \mathbb{R}^{Ah}$, which directly outputs an action from Gaussian noise in one network forward pass. This noise-conditioned policy is trained to maximize the Q-chunking critic $Q_\theta(s_t, \ac{t}{t+h})$ while being regularized to be close to the behavioral cloning flow-matching policy via a distillation loss that is shown to be an upper-bound on the square 2-Wasserstein distance~\citep{park2025flow}:
\begin{align}
    L(\psi) &= \mathbb{E}_{s_t \sim \mathcal{D}, \mathbf z^0 \sim \mathcal{N}(0, \mathbf{I}_{Ah})}\left[{\color{ourblue}\alpha\left\|\mathbf z^1 - \mu_\psi(s_t, \mathbf z^0)\right\|_2^2} - Q(s_t, \mu_\psi(s_t, \mathbf z))\right] 
    \label{eq:fql-onestep}\\
    & \geq \mathbb{E}_{s_t \sim \mathcal{D}, \mathbf z^0 \sim \mathcal{N}(0, \mathbf{I}_{Ah})}\left[{\color{ourblue}\alpha W_2(\pi_\psi(\cdot | s_t), f_\xi(\cdot | s_t))^2} - Q(s_t, \mu_\psi(s_t, \mathbf z))\right],
\end{align}
where $\mathbf z^1$ is the ODE solution from $u=0$ to $u=1$ following $\dd \mathbf z^u = f_\xi(s_t, \mathbf z^u, u) \dd u$ (the initial value $\mathbf z^0$ is sampled from the unit Gaussian). The real-valued hyperparameter $\alpha$ directly controls the magnitude of the distillation loss. Finally, the TD loss remains the same as the previous section with the only difference in how we parameterize the policy:
\begin{align}
    L(\theta) = \mathbb{E}_{s_t, \mathbf a_t, s_{t+h} \sim \mathcal{D}, {\color{ourblue}\mathbf z}}\left[\left(Q_\theta(s_t, \mathbf a_t) - \sum_{t'=1}^{h}\gamma^{t'} r_{t+t'} - \gamma^h Q_{\bar \theta}(s_{t+h}, {\color{ourblue} \mu_\psi(s_{t+h}, \mathbf z)})\right)^2\right]
    \label{eq:fql-critic}
\end{align}
where again $\mathbf z \sim \mathcal{N}(0, \mathbf I_{Ah})$.


\textbf{Offline-to-online RL considerations.} Since both variants of our methods use beahvior constraint (implicit KL for QC, explicit $W_2$ for QC-FQL), we can also directly run them for offline RL pre-training, which provides further sample efficiency gain. For both offline and online training, we use the same behavior constraint strength (e.g., $N$ for QC and $\alpha$ for QC-FQL). See Algorithm \ref{algo:qc} and \ref{algo:qc-fql} for an overview of QC and QC-FQL during online training. For offline training, we use the same algorithm and simply remove the environment interaction part. 





\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
\caption{{\color{ourpurple}$\mathtt{QC}$}}
\label{algo:qc}
\begin{algorithmic}

\small
\State \textbf{Input:} Behavior policy and critic: $f_\xi(\ac{t}{t+h} | s)$ and $Q_\theta(s_t, \ac{t}{t+h})$.
\State $\mathcal{D} \leftarrow$ offline prior data.
\For{every environment step $t$}
    \If{$t \mod h \equiv 0$}
        \State $\ac{t}{t+h}^1 \cdots \ac{t}{t+h}^N \sim f_\xi(\cdot | s_t)$
        \State $\ac{t}{t+h}^\star \leftarrow \arg\max_{\ac{t}{t+h}^i} Q_\theta(s, \ac{t}{t+h}^i)$
    \EndIf
    \State Act with $a^\star_t$ and receive $s_{t+1}, r_t$.
    \State $\mathcal{D} \leftarrow \mathcal{D} \cup \{(s_t, a^\star_t, s_{t+1}, r_t)\}$
    \State Update $f_\xi$ via flow-matching loss using $\mathcal{D}$ .
    \State Update $Q_\theta$ via Eq (\ref{eq:qc-critic}) using $\mathcal{D}$.
\EndFor
\State \textbf{Output:} $f_\xi, Q_\theta$.
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
\caption{{\color{ourpurple}$\mathtt{QC}$-$\mathtt{FQL}$}}
\label{algo:qc-fql}
\begin{algorithmic}
\small
\State \textbf{Input:} Behavior policy, critic, one-step policy: $f_\xi(\ac{t}{t+h} | s)$, $Q_\theta(s_t, \ac{t}{t+h})$, $\mu_\psi(s, \mathbf z)$.
\State $\mathcal{D} \leftarrow$ offline prior data.
\For{every environment step $t$}
    \If{$t \mod h  \equiv 0$}
        \State $\mathbf z \sim \mathcal{N}(0, \mathbf I_{Ah})$
        \State $\ac{t}{t+h} \leftarrow \mu_\psi(s_t, \mathbf z)$
    \EndIf
    \State Act with $a_t$ and receive $s_{t+1}, r_t$.
    \State $\mathcal{D} \leftarrow \mathcal{D} \cup \{(s_t, a_t, s_{t+1}, r_t)\}$
    \State Update $f_\xi$ via flow-matching loss using $\mathcal{D}$.
    \State Update $\mu_{\psi}$ and $Q_\theta$ via Eq. (\ref{eq:fql-onestep}, \ref{eq:fql-critic}) using $\mathcal{D}$.
\EndFor
\State \textbf{Output:} $f_\xi, Q_\theta, \mu_\psi(s, \mathbf z)$.
\end{algorithmic}
\end{algorithm}
\end{minipage}

\definecolor{ddblue}{HTML}{4592d7}
\definecolor{lblue}{HTML}{cfe2f3}
\definecolor{lred}{rgb}{1.0, 0.9, 0.9}














\section{Experimental Results}
\label{sec:results}
We conduct a series of experiments to analyze the empirical effectiveness of our method on a range of long-horizon, sparse-reward domains. In particular, we are going to answer the following questions:
\begin{enumerate}[start=1,label={(\bfseries Q\arabic*)}]
    \item \emph{How well do Q-chunking methods perform compared to prior offline-to-online RL methods?}
    \item \emph{Why does action chunking helps online learning?}
    \item \emph{How does chunk length, critic ensemble size, and update-to-data ratio affect performance?}
\end{enumerate}

\subsection{Environments and Datasets}
We first consider six sparse reward robotic manipulation domains with tasks of varying difficulties. This includes 5 domains from OGBench~\citep{ogbench_park2024}, \texttt{scene-sparse}, \texttt{puzzle-3x3-sparse}, \texttt{cube-double/triple/quadruple} (5 tasks each) and 3 tasks in the robomimic benchmark~\citep{robomimic2021}. For OGBench, we use the default play-style datasets except for \texttt{cube-quadruple} where we use a large 100M-size dataset. For robomimic, we use the multi-human datasets. See more details of these environments and datasets in Appendix \ref{appendix:domain}.






\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/main-all-ablation.pdf} %
    \caption{\footnotesize \textbf{Aggregated performance per OGBench domain.} Our method, \hourpurple{QC}, achieves strong performance across all five challenging OGBench domains. We also include an aggregation performance plot for all the domains at the bottom right. The first 1M steps are offline training and the next 1M steps are online training with one environment step per training step (4 seeds per task; 5 tasks per domain).}
    \label{fig:main}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/robomimic-all-ablation.pdf} %
    \caption{\footnotesize \textbf{Robomimic results.} \hourpurple{QC} achieves strong performance across all three robomimic tasks. The first 1M steps are offline and the next 1M steps are online with one environment step per training step (5 seeds).}
    \label{fig:main-robomimic}
\end{figure}



\subsection{Comparisons}
We primarily compare with prior methods that speedup value backup as well as the previous best offline-to-online RL methods.

\hourblue{BFN (best-of-$N$)} is a baseline that we propose to combine the expected-max $Q$ operator~\citep{ghasemipour2021emaq} with an expressive behavior flow policy. BFN operates in the original action space and uses best-of-$N$ sampling to pick the action (out of $N$) that maximizes the current $Q$-value. This baseline is an ablation to isolate the benefit of Q-chunking in \hourpurple{QC}.

\hourblue{FQL}~\citep{park2025flow} is a recently proposed offline RL method that achieves strong offline and offline-to-online RL performance. 
This baseline is an ablation to isolate the benefit of Q-chunking in \hourpurple{QC-FQL}.

\hourmiddle{BFN-n/FQL-n.} These baselines are the same as BFN/FQL but uses $n$-step backup with $n=1$ (Equation \ref{eq:nstep}) instead of the standard 1-step TD backup. This baseline enjoys the benefits of value backup speedup, but does not use chunked critic or actor, and potentially suffer from the bias issue.

\hourblue{RLPD}~\citep{ball2023efficient}, \hourblue{RLPD-AC}. RLPD is a sample-efficient RL algorithm that treats offline data as additional off-policy data and learn from scratch online. 
RLPD-AC is the same as RLPD but operates on the temporally extended action space. Both of them do not use a behavior constraint.


\begin{figure}
    \centering
    \includegraphics[width=0.48\linewidth]{figures/agg.pdf}  %
    \hfill
    \includegraphics[width=0.48\linewidth]{figures/robomimic-agg.pdf} %
    \caption{\footnotesize \textbf{\hourpurple{QC-FQL} and $n$-step return on OGBench and robomimic.} \hourpurple{QC-FQL} obtains a similar performance compared to \hourpurple{QC}. QC is slightly better than QC-FQL on OGBench offline and robomimic online, and slightly worse than QC-FQL on robomimic offline (4 seeds for OGBench, 5 seeds for robomimic). See Appendix \ref{appendix:results}, Figure \ref{fig:ablation-all-individual} and \ref{fig:full-robomimic} for full results.}
    \label{fig:ablation}
\end{figure}

See more implementation details of our method and the baselines in Appendix \ref{appendix:impl-details}. %



















\subsection{How well does our method compare to prior offline-to-online RL methods?}
\label{sec:results-analysis}
We report the aggregated performance of Q-chunking and the baselines for each of the five OGBench domains (\cref{fig:main}) and individual performance on three robomimic tasks (\cref{fig:main-robomimic}). \hourpurple{QC} achieves competitive performance offline (in grey), often matching or sometimes outperforming best prior methods. In the online phase (in white), \hourpurple{QC} shows strong sample-efficiency, especially on the two hardest OGBench domains (\texttt{cube-triple/quadruple}), where it outperforms all prior methods (especially on \texttt{cube-quadruple}) by a large margin. We also conduct an ablation study where we compare \hourpurple{QC} with a variant of our method \hourpurple{QC-FQL} and two $n$-step return baselines (\hourmiddle{BFN-n} and \hourmiddle{FQL-n}) in Figure \ref{fig:ablation}. The $n$-step return baselines, which do not leverage a temporally extended critic or policy, perform significantly worse than our methods (\hourpurple{QC} and \hourpurple{QC-FQL}). In fact, they often underperform even the 1-step baselines (\hourblue{BFN} and \hourblue{FQL}), highlighting the importance of learning in the temporally extended action space.















\begin{figure}[t]
    \includegraphics[width=0.33\linewidth]{figures/ac-size.pdf} %
    \includegraphics[width=0.33\linewidth]{figures/10qs.pdf} 
    \includegraphics[width=0.33\linewidth]{figures/utd.pdf} %
    \caption{\footnotesize \textbf{Sensitivity analysis: action chunk size ($h$), critic ensemble size ($K$), and update-to-data ratio (UTD).} \emph{Left:} \hourpurple{QC-FQL} with different $h$ on all 5 \texttt{cube-triple} tasks (5 seeds). QC-FQL with $h=1$ is equivalent to \hourblue{FQL}. \emph{Center:} Increasing the ensemble size to $K=10$ improves performance of both \hourpurple{QC} and \hourblue{BFN} on \texttt{cube-triple-task3} (5 seeds). \emph{Right:} \hourpurple{QC} with UTD of 5 on \texttt{cube-triple-task3} (5 seeds). We report only the online phase results, as all methods achieve near-zero success rates during the offline phase.}
    \label{fig:sensitivity}
\end{figure}
\begin{figure}[t]
    \centering
\includegraphics[width=0.52\linewidth]{figures/traj_viz.pdf}
\includegraphics[width=0.47\linewidth]{figures/eefd.pdf} %

\caption{\footnotesize \textbf{End-effector movements {\color{gray}early} in the training and temporal coherency analysis on \texttt{cube-triple-task3}.} \emph{Left:} \hourpurple{QC} covers a more diverse set of states compared to \hourblue{BFN} in the first 1000 environment steps. \emph{Right:} \hourpurple{QC} exhibits a higher temporal coherency in end-effector compared to \hourblue{BFN} (4 seeds).}
\label{fig:eef}
\end{figure}

    



\subsection{Why does action chunking help exploration?}
We hypothesize in Section \ref{sec:qc-p} that action chunking policy produce more temporally coherent actions and thus lead to better state coverage and exploration. In this section, we study to what degree that holds empirically. We first visualize the end-effector movements early in the training for \hourpurple{QC} and \hourblue{BFN} (Figure \ref{fig:eef}, left). \hourblue{BFN}'s trajectory contains many pauses (as indicated by a very big and dense cluster near the center of the visualization), especially when the end-effector is being lowered to pickup a cube. In contrast, \hourpurple{QC} has fewer pauses (fewer and shallower clusters) and a more diverse state coverage in the end-effector space. We include additional examples in Appendix \ref{appendix:results}, Figure \ref{fig:traj-viz-early} and Figure \ref{fig:traj-viz-late}. To get a quantitative measure of the temporal coherency in actions, we record the 3-D end-effector position throughout training every 5 time steps: $\{\mathbf{x}^{\mathrm{eef}}_0, \mathbf{x}^{\mathrm{eef}}_5, \cdots\}$ and compute the average $L_2$ norm of the difference vector of two adjacent end-effector positions. This average norm would be small if there are any pauses or jittery motions, making a good proxy for measuring the temporal coherency in actions. 
As shown in Figure \ref{fig:eef} (right), \hourpurple{QC} exhibits a higher action temporal coherency throughout training compared to \hourblue{BFN}. This suggests that Q-chunking improves temporal coherency in actions, which explains the improved sample-efficiency that Q-chunking brings. 

\subsection{How does action chunk length, critic ensemble size affect the performance of our method?}
In Figure \ref{fig:sensitivity} (left), we report the performance of \hourpurple{QC-FQL} with different action chunk sizes ($h \in \{1, 5, 10, 25\}$) on the \texttt{cube-triple} domain. In general, a higher action chunk length helps but not significantly. We use $h=5$ in all our other experiments as $h=5$ is cheap to run. In Figure \ref{fig:sensitivity} (center), we study how the critic ensemble size affects the performance of our method. Using 10 critics improves both \hourpurple{QC} and \hourblue{BFN}. We use $K=2$ in our other experiments as it is cheap to run. Using $K=10$ could potentially make Q-chunking perform much better on the benchmark tasks we consider. Finally, we observe that increasing the update-to-data ratio (UTD) does not improve the sample efficiency of \hourpurple{QC} (Figure \ref{fig:sensitivity}, right). 


\section{Discussions}
\label{sec:discussions}
We demonstrate how action chunking can be integrated into an offline-to-online RL agent with a simple recipe. Our approach speeds up value backup and explores more effectively online with temporally coherent actions. As a result, it outperforms prior offline-to-online methods on a range of challenging long-horizon tasks. Our work serves as a step towards training non-Markovian policy for effective online exploration from prior offline data. Several challenges remain, opening promising directions for future research. First, our approach use a fixed action chunk, but it is unclear how to choose this size other than task-specific hyperparameter tuning. A natural next step would be to develop mechanisms that automatically determine chunk boundaries. Second, action chunking represents only a limited subclass of non-Markovian policies and may perform poorly in settings where a high-frequency control feedback loop is essential. Developing practical techniques for training more general non-Markovian policies for online exploration would further improve the online sample efficiency of offline-to-online RL algorithms. 


