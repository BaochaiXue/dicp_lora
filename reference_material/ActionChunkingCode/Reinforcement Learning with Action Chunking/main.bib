@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2008},
  publisher={Springer}
}

@article{de2025learning,
  title={Learning Transferable Sub-Goals by Hypothesizing Generalizing Features},
  author={de Mello Koch, Anita and Bagaria, Akhil and Huo, Bingnan and Zhou, Zhiyuan and Allen, Cameron and Konidaris, George},
  year={2025}
}

@article{pinsker1964information,
  title={Information and information stability of random variables and processes},
  author={Pinsker, Mark S},
  journal={Holden-Day},
  year={1964}
}

@article{wurman2022outracing,
  title={Outracing champion Gran Turismo drivers with deep reinforcement learning},
  author={Wurman, Peter R and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and others},
  journal={Nature},
  volume={602},
  number={7896},
  pages={223--228},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation.},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder},
  booktitle={ICML},
  volume={2000},
  pages={759--766},
  year={2000},
  organization={Citeseer}
}

@inproceedings{kozuno2021revisiting,
  title={Revisiting Pengâ€™s Q ($\lambda$) for Modern Reinforcement Learning},
  author={Kozuno, Tadashi and Tang, Yunhao and Rowland, Mark and Munos, R{\'e}mi and Kapturowski, Steven and Dabney, Will and Valko, Michal and Abel, David},
  booktitle={International Conference on Machine Learning},
  pages={5794--5804},
  year={2021},
  organization={PMLR}
}

@inproceedings{kapturowski2018recurrent,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={International conference on learning representations},
  year={2018}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PmLR}
}

@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby and others},
  year={1989},
  publisher={King's College, Cambridge United Kingdom}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle={International conference on machine learning},
  pages={3061--3071},
  year={2020},
  organization={PMLR}
}

@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@inproceedings{ghasemipour2021emaq,
  title={Emaq: Expected-max q-learning operator for simple yet effective offline and online rl},
  author={Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  booktitle={International Conference on Machine Learning},
  pages={3682--3691},
  year={2021},
  organization={PMLR}
}

@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{tian2025chunking,
  title={Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns},
  author={Tian, Dong and Li, Ge and Zhou, Hongyi and Celik, Onur and Neumann, Gerhard},
  journal={arXiv preprint arXiv:2503.03660},
  year={2025}
}

@article{kouvaritakis2016model,
  title={Model predictive control},
  author={Kouvaritakis, Basil and Cannon, Mark},
  journal={Switzerland: Springer International Publishing},
  volume={38},
  number={13-56},
  pages={7},
  year={2016},
  publisher={Springer}
}

@article{li2024top,
  title={TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning},
  author={Li, Ge and Tian, Dong and Zhou, Hongyi and Jiang, Xinkai and Lioutikov, Rudolf and Neumann, Gerhard},
  journal={arXiv preprint arXiv:2410.09536},
  year={2024}
}

@inproceedings{bharadhwaj2024roboagent,
  title={Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking},
  author={Bharadhwaj, Homanga and Vakil, Jay and Sharma, Mohit and Gupta, Abhinav and Tulsiani, Shubham and Kumar, Vikash},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={4788--4795},
  year={2024},
  organization={IEEE}
}

@article{george2023one,
  title={One act play: Single demonstration behavior cloning with action chunking transformers},
  author={George, Abraham and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2309.10175},
  year={2023}
}

@article{wang2023train,
  title={Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning},
  author={Wang, Shenzhi and Yang, Qisen and Gao, Jiawei and Lin, Matthieu and Chen, Hao and Wu, Liwei and Jia, Ning and Song, Shiji and Huang, Gao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={47081--47104},
  year={2023}
}

@article{luo2023finetuning,
  title={Finetuning from offline reinforcement learning: Challenges, trade-offs and practical solutions},
  author={Luo, Yicheng and Kay, Jackie and Grefenstette, Edward and Deisenroth, Marc Peter},
  journal={arXiv preprint arXiv:2303.17396},
  year={2023}
}

@article{agarwal2022reincarnating,
  title={Reincarnating reinforcement learning: Reusing prior computation to accelerate progress},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={28955--28971},
  year={2022}
}

@article{zhou2024efficient,
  title={Efficient online reinforcement learning fine-tuning need not retain offline data},
  author={Zhou, Zhiyuan and Peng, Andy and Li, Qiyang and Levine, Sergey and Kumar, Aviral},
  journal={arXiv preprint arXiv:2412.07762},
  year={2024}
}

@article{wilcoxson2024leveraging,
  title={Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration},
  author={Wilcoxson, Max and Li, Qiyang and Frans, Kevin and Levine, Sergey},
  journal={arXiv preprint arXiv:2410.18076},
  year={2024}
}

@article{ren2024diffusion,
  title={Diffusion policy policy optimization},
  author={Ren, Allen Z and Lidard, Justin and Ankile, Lars L and Simeonov, Anthony and Agrawal, Pulkit and Majumdar, Anirudha and Burchfiel, Benjamin and Dai, Hongkai and Simchowitz, Max},
  journal={arXiv preprint arXiv:2409.00588},
  year={2024}
}


@inproceedings{robomimic2021,
  title={What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
  author={Ajay Mandlekar and Danfei Xu and Josiah Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Li Fei-Fei and Silvio Savarese and Yuke Zhu and Roberto Mart\'{i}n-Mart\'{i}n},
  booktitle={arXiv preprint arXiv:2108.03298},
  year={2021}
}

@article{park2024ogbench,
  title={Ogbench: Benchmarking offline goal-conditioned rl},
  author={Park, Seohong and Frans, Kevin and Eysenbach, Benjamin and Levine, Sergey},
  journal={arXiv preprint arXiv:2410.20092},
  year={2024}
}

@article{psenka2023learning,
  title={Learning a diffusion model policy from rewards via q-score matching},
  author={Psenka, Michael and Escontrela, Alejandro and Abbeel, Pieter and Ma, Yi},
  journal={arXiv preprint arXiv:2312.11752},
  year={2023}
}

@article{fang2024diffusion,
  title={Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning},
  author={Fang, Linjiajie and Liu, Ruoxue and Zhang, Jing and Wang, Wenjia and Jing, Bing-Yi},
  journal={arXiv preprint arXiv:2405.20555},
  year={2024}
}

@article{mark2024policy,
  title={Policy agnostic rl: Offline rl and online rl fine-tuning of any class and backbone},
  author={Mark, Max Sobol and Gao, Tian and Sampaio, Georgia Gabriela and Srirama, Mohan Kumar and Sharma, Archit and Finn, Chelsea and Kumar, Aviral},
  journal={arXiv preprint arXiv:2412.06685},
  year={2024}
}

@article{li2024learning,
  title={Learning multimodal behaviors from scratch with diffusion policy gradient},
  author={Li, Steven and Krohn, Rickmer and Chen, Tao and Ajay, Anurag and Agrawal, Pulkit and Chalvatzaki, Georgia},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={38456--38479},
  year={2024}
}

@article{durugkar2021adversarial,
  title={Adversarial intrinsic motivation for reinforcement learning},
  author={Durugkar, Ishan and Tec, Mauricio and Niekum, Scott and Stone, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8622--8636},
  year={2021}
}

@article{degrave2019quinoa,
  title={Quinoa: a Q-function you infer normalized over actions},
  author={Degrave, Jonas and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1911.01831},
  year={2019}
}

@inproceedings{touati2022does,
  title={Does Zero-Shot Reinforcement Learning Exist?},
  author={Touati, Ahmed and Rapin, J{\'e}r{\'e}my and Ollivier, Yann},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{bagaria2019option,
  title={Option discovery using deep skill chaining},
  author={Bagaria, Akhil and Konidaris, George},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@techreport{csimcsek2007betweenness,
title = "Betweenness Centrality as a Basis for Forming Skills",
abstract = "We show that betweenness centrality, a graph-theoretic measure widely used in social network analysis, provides a sound basis for autonomously forming useful high-level behaviors, or skills, from available primitivesâ€” the smallest behavioral units available to an autonomous agent.",
keywords = "reinforcement learning, skill discovery, Hierarchical reinforcement learning, action hierachy",
author = "{\"O}zg{\"u}r {\c S}im{\c s}ek and Barto, {Andrew G.}",
year = "2007",
month = apr,
day = "12",
language = "English",
publisher = "University of Massachusetts Amherst",
type = "WorkingPaper",
institution = "University of Massachusetts Amherst",
}

@inproceedings{
    touati2023does,
    title={Does Zero-Shot Reinforcement Learning Exist?},
    author={Ahmed Touati and J{\'e}r{\'e}my Rapin and Yann Ollivier},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=MYEap_OcQI}
}

@article{achiam2017surprise,
  title={Surprise-based intrinsic motivation for deep reinforcement learning},
  author={Achiam, Joshua and Sastry, Shankar},
  journal={arXiv preprint arXiv:1703.01732},
  year={2017}
}

@article{achiam2018variational,
  title={Variational option discovery algorithms},
  author={Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1807.10299},
  year={2018}
}

@inproceedings{NEURIPS2022_ba1c5356,
 author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {28955--28971},
 publisher = {Curran Associates, Inc.},
 title = {Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress},
 volume = {35},
 year = {2022}
}

@inproceedings{
ajay2020opal,
title={{OPAL}: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning},
author={Anurag Ajay and Aviral Kumar and Pulkit Agrawal and Sergey Levine and Ofir Nachum},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=V69LGwJ0lIN}
}

@inproceedings{bacon2017option,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{srinivas2016option,
  title={Option discovery in hierarchical reinforcement learning using spatio-temporal clustering},
  author={Srinivas, Aravind and Krishnamurthy, Ramnandan and Kumar, Peeyush and Ravindran, Balaraman},
  journal={arXiv preprint arXiv:1605.05359},
  year={2016}
}

@article{bagaria2024effectively,
  title={Effectively learning initiation sets in hierarchical reinforcement learning},
  author={Bagaria, Akhil and Abbatematteo, Ben and Gottesman, Omer and Corsaro, Matt and Rammohan, Sreehari and Konidaris, George},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1577--1594},
  year={2023},
  organization={PMLR}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}

@inproceedings{gpt3_brown2020,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@article{chen2024self,
  title={Self-supervised reinforcement learning that transfers using random features},
  author={Chen, Boyuan and Zhu, Chuning and Agrawal, Pulkit and Zhang, Kaiqing and Gupta, Abhishek},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{cho2014learning,
  title={Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{daniel2016hierarchical,
  title={Hierarchical relative entropy policy search},
  author={Daniel, Christian and Neumann, Gerhard and Kroemer, Oliver and Peters, Jan},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={93},
  pages={1--50},
  year={2016}
}

@article{daniel2016probabilistic,
  title={Probabilistic inference for determining options in reinforcement learning},
  author={Daniel, Christian and Van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
  journal={Machine Learning},
  volume={104},
  pages={337--357},
  year={2016},
  publisher={Springer}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ermolov2020latent,
  title={Latent world models for intrinsically motivated exploration},
  author={Ermolov, Aleksandr and Sebe, Nicu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5565--5575},
  year={2020}
}

@article{eysenbach2018diversity,
  title={Diversity is all you need: Learning skills without a reward function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}


@inproceedings{
    florensa2017stochastic,
    title={Stochastic Neural Networks for Hierarchical Reinforcement Learning},
    author={Carlos Florensa and Yan Duan and Pieter Abbeel},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=B1oK8aoxe}
}

@article{fox2017multi,
  title={Multi-level discovery of deep options},
  author={Fox, Roy and Krishnan, Sanjay and Stoica, Ion and Goldberg, Ken},
  journal={arXiv preprint arXiv:1703.08294},
  year={2017}
}

@InProceedings{frans2024unsupervised,
  title = 	 {Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings},
  author =       {Frans, Kevin and Park, Seohong and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {13927--13942},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/frans24a/frans24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/frans24a.html},
  abstract = 	 {Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a <em>functional</em> reward encoding (FRE) as a general, scalable solution to this <em>zero-shot RL</em> problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods.}
}

@article{fu2020d4rl,
  title={{D4RL}: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@inproceedings{ghosh2023reinforcement,
  title={Reinforcement learning from passive data via latent intentions},
  author={Ghosh, Dibya and Bhateja, Chethan Anand and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={11321--11339},
  year={2023},
  organization={PMLR}
}

@article{gregor2016variational,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1611.07507},
  year={2016}
}

@article{guo2022byol,
  title={Byol-explore: Exploration by bootstrapped prediction},
  author={Guo, Zhaohan and Thakoor, Shantanu and P{\^\i}slar, Miruna and Avila Pires, Bernardo and Altch{\'e}, Florent and Tallec, Corentin and Saade, Alaa and Calandriello, Daniele and Grill, Jean-Bastien and Tang, Yunhao and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={31855--31870},
  year={2022}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@inproceedings{
    hansen2019fast,
    title={Fast Task Inference with Variational Intrinsic Successor Features},
    author={Steven Hansen and Will Dabney and Andre Barreto and David Warde-Farley and Tom Van de Wiele and Volodymyr Mnih},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BJeAHkrYDS}
}

@article{hansen2023idql,
  title={{IDQL}: Implicit {Q}-learning as an actor-critic method with diffusion policies},
  author={Hansen-Estruch, Philippe and Kostrikov, Ilya and Janner, Michael and Kuba, Jakub Grudzien and Levine, Sergey},
  journal={arXiv preprint arXiv:2304.10573},
  year={2023}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{houthooft2016vime,
  title={{VIME}: Variational information maximizing exploration},
  author={Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1109--1117},
  year={2016}
}

@inproceedings{
hu2023unsupervised,
title={Unsupervised Behavior Extraction via Random Intent Priors},
author={Hao Hu and Yiqin Yang and Jianing Ye and Ziqing Mai and Chongjie Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=4vGVQVz5KG}
}

@article{jiang2022efficient,
  title={Efficient planning in a compact latent action space},
  author={Jiang, Zhengyao and Zhang, Tianjun and Janner, Michael and Li, Yueying and Rockt{\"a}schel, Tim and Grefenstette, Edward and Tian, Yuandong},
  journal={arXiv preprint arXiv:2208.10291},
  year={2022}
}

@article{kim2019variational,
  title={Variational temporal abstraction},
  author={Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@book{konidaris2011autonomous,
  title={Autonomous robot skill acquisition},
  author={Konidaris, George Dimitri},
  year={2011},
  publisher={University of Massachusetts Amherst}
}

@article{kostrikov2021offline,
  title={Offline reinforcement learning with implicit {Q}-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{kumar2020conservative,
  title={Conservative {Q}-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@inproceedings{lee2022offline,
  title={Offline-to-online reinforcement learning via balanced replay and pessimistic {Q}-ensemble},
  author={Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  booktitle={Conference on Robot Learning},
  pages={1702--1712},
  year={2022},
  organization={PMLR}
}

@inproceedings{li2023understanding,
  title={Understanding the complexity gains of single-task {RL} with a curriculum},
  author={Li, Qiyang and Zhai, Yuexiang and Ma, Yi and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={20412--20451},
  year={2023},
  organization={PMLR}
}

@article{li2024accelerating,
  title={Accelerating exploration with unlabeled prior data},
  author={Li, Qiyang and Zhang, Jason and Ghosh, Dibya and Zhang, Amy and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
    Liu2021BehaviorFT,
    title={Behavior From the Void: Unsupervised Active Pre-Training},
    author={Hao Liu and Pieter Abbeel},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=fIn4wLS2XzU}
}

@inproceedings{lobel2023flipping,
  title={Flipping coins to estimate pseudocounts for exploration in reinforcement learning},
  author={Lobel, Sam and Bagaria, Akhil and Konidaris, George},
  booktitle={International Conference on Machine Learning},
  pages={22594--22613},
  year={2023},
  organization={PMLR}
}

@inproceedings{mannor2004dynamic,
  title={Dynamic abstraction in reinforcement learning via clustering},
  author={Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={71},
  year={2004}
}

@inproceedings{menache2002q,
  title={{Q}-cutâ€”dynamic discovery of sub-goals in reinforcement learning},
  author={Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
  booktitle={Machine Learning: ECML 2002: 13th European Conference on Machine Learning Helsinki, Finland, August 19--23, 2002 Proceedings 13},
  pages={295--306},
  year={2002},
  organization={Springer}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@article{nakamoto2024cal,
  title={{Cal-QL}: Calibrated offline {RL} pre-training for efficient online fine-tuning},
  author={Nakamoto, Mitsuhiko and Zhai, Simon and Singh, Anikait and Sobol Mark, Max and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{nasiriany2022learning,
  title={Learning and Retrieval from Prior Data for Skill-based Imitation Learning},
  author={Nasiriany, Soroush and Gao, Tian and Mandlekar, Ajay and Zhu, Yuke},
  booktitle={Conference on Robot Learning},
  year={2022}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={2721--2730},
  year={2017},
  organization={PMLR}
}

@inproceedings{
    park2023hiql,
    title={{HIQL}: Offline Goal-Conditioned {RL} with Latent States as Actions},
    author={Seohong Park and Dibya Ghosh and Benjamin Eysenbach and Sergey Levine},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=cLQCCtVDuW}
}

@inproceedings{
    park2023metra,
    title={{METRA}: Scalable Unsupervised {RL} with Metric-Aware Abstraction},
    author={Seohong Park and Oleh Rybkin and Sergey Levine},
    booktitle={NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning},
    year={2023},
    url={https://openreview.net/forum?id=YgZNmDqyR6}
}

@inproceedings{
    park2024foundation,
    title={Foundation Policies with Hilbert Representations},
    author={Seohong Park and Tobias Kreiman and Sergey Levine},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=LhNsSaAKub}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@inproceedings{pertsch2021accelerating,
  title={Accelerating reinforcement learning with learned skill priors},
  author={Pertsch, Karl and Lee, Youngwoon and Lim, Joseph},
  booktitle={Conference on robot learning},
  pages={188--204},
  year={2021},
  organization={PMLR}
}

@article{gpt2_radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{shankar2020learning,
  title={Learning robot skills with temporal variational inference},
  author={Shankar, Tanmay and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={8624--8633},
  year={2020},
  organization={PMLR}
}

@inproceedings{
sharma2019dynamics,
title={Dynamics-Aware Unsupervised Discovery of Skills},
author={Archit Sharma and Shixiang Gu and Sergey Levine and Vikash Kumar and Karol Hausman},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJgLZR4KvH}
}

@inproceedings{csimcsek2004using,
  title={Using relative novelty to identify useful temporal abstractions in reinforcement learning},
  author={{\c{S}}im{\c{s}}ek, {\"O}zg{\"u}r and Barto, Andrew G},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={95},
  year={2004}
}

@inproceedings{simsek2008skill,
  title={Skill characterization based on betweenness},
  author={Simsek, Ozgur and Barreto, Andre S},
  booktitle={Advances in neural information processing systems},
  pages={1497--1504},
  year={2008}
}

@inproceedings{
    singh2020parrot,
    title={Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},
    author={Avi Singh and Huihan Liu and Gaoyue Zhou and Albert Yu and Nicholas Rhinehart and Sergey Levine},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=Ysuv-WOFeKR}
}

@inproceedings{
    song2022hybrid,
    title={Hybrid {RL}: Using both offline and online data can make {RL} efficient},
    author={Yuda Song and Yifei Zhou and Ayush Sekhari and Drew Bagnell and Akshay Krishnamurthy and Wen Sun},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=yyBis80iUuU}
}

@article{stadie2015incentivizing,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}

@article{sutton1999between,
  title={Between {MDPs} and {semi-MDPs}: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{tang2017exploration,
  title={\# exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{tarasov2024revisiting,
  title={Revisiting the minimalist approach to offline reinforcement learning},
  author={Tarasov, Denis and Kurenkov, Vladislav and Nikulin, Alexander and Kolesnikov, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@inproceedings{uchendu2023jump,
  title={Jump-start reinforcement learning},
  author={Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Jos{\'e}phine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and others},
  booktitle={International Conference on Machine Learning},
  pages={34556--34583},
  year={2023},
  organization={PMLR}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@inproceedings{
zhang2023policy,
title={Policy Expansion for Bridging Offline-to-Online Reinforcement Learning},
author={Haichao Zhang and Wei Xu and Haonan Yu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=-Y34L45JR6z}
}

@inproceedings{zheng2023adaptive,
  title={Adaptive policy learning for offline-to-online reinforcement learning},
  author={Zheng, Han and Luo, Xufang and Wei, Pengfei and Song, Xuan and Li, Dongsheng and Jiang, Jing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11372--11380},
  year={2023}
}

@article{ogbench_park2024,
  title={OGBench: Benchmarking Offline Goal-Conditioned RL},
  author={Seohong Park and Kevin Frans and Benjamin Eysenbach and Sergey Levine},
  journal={ArXiv},
  year={2024}
}

@article{dalal2021accelerating,
  title={Accelerating robotic reinforcement learning via parameterized action primitives},
  author={Dalal, Murtaza and Pathak, Deepak and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21847--21859},
  year={2021}
}

@article{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{nikishin2023deep,
  title={Deep reinforcement learning with plasticity injection},
  author={Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andr{\'e}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={37142--37159},
  year={2023}
}


@article{gehring2021hierarchical,
  title={Hierarchical skills for efficient exploration},
  author={Gehring, Jonas and Synnaeve, Gabriel and Krause, Andreas and Usunier, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11553--11564},
  year={2021}
}

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}

@inproceedings{
xie2021latent,
title={Latent Skill Planning for Exploration and Transfer},
author={Kevin Xie and Homanga Bharadhwaj and Danijar Hafner and Animesh Garg and Florian Shkurti},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=jXe91kq3jAq}
}

@article{paraschos2013probabilistic,
  title={Probabilistic movement primitives},
  author={Paraschos, Alexandros and Daniel, Christian and Peters, Jan R and Neumann, Gerhard},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}

@article{peng2017deeploco,
  title={Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning},
  author={Peng, Xue Bin and Berseth, Glen and Yin, KangKang and Van De Panne, Michiel},
  journal={Acm transactions on graphics (tog)},
  volume={36},
  number={4},
  pages={1--13},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{merel2018neural,
  title={Neural probabilistic motor primitives for humanoid control},
  author={Merel, Josh and Hasenclever, Leonard and Galashov, Alexandre and Ahuja, Arun and Pham, Vu and Wayne, Greg and Teh, Yee Whye and Heess, Nicolas},
  journal={arXiv preprint arXiv:1811.11711},
  year={2018}
}

@article{dayan1992feudal,
  title={Feudal reinforcement learning},
  author={Dayan, Peter and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@inproceedings{riedmiller2018learning,
  title={Learning by playing solving sparse reward tasks from scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle={International conference on machine learning},
  pages={4344--4353},
  year={2018},
  organization={PMLR}
}

@article{chentanez2004intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew and Singh, Satinder},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@article{oh2017value,
  title={Value prediction network},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}

@article{seo2024reinforcement,
  title={Reinforcement Learning with Action Sequence for Data-Efficient Robot Learning},
  author={Seo, Younggyo and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2411.12155},
  year={2024}
}

@article{vezhnevets2016strategic,
  title={Strategic attentive writer for learning macro-actions},
  author={Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{park2025flow,
  title={Flow {Q}-Learning},
  author={Park, Seohong and Li, Qiyang and Levine, Sergey},
  journal={arXiv preprint arXiv:2502.02538},
  year={2025}
}

@article{vuong2022dasco,
  title={Dasco: Dual-generator adversarial support constrained offline reinforcement learning},
  author={Vuong, Quan and Kumar, Aviral and Levine, Sergey and Chebotar, Yevgen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38937--38949},
  year={2022}
}

@article{chao2024maximum,
  title={Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow},
  author={Chao, Chen-Hao and Feng, Chien and Sun, Wei-Fang and Lee, Cheng-Kuang and See, Simon and Lee, Chun-Yi},
  journal={arXiv preprint arXiv:2405.13629},
  year={2024}
}

@article{kordabad2022safe,
  title={Safe reinforcement learning using Wasserstein distributionally robust MPC and chance constraint},
  author={Kordabad, Arash Bahari and Wisniewski, Rafael and Gros, Sebastien},
  journal={IEEE Access},
  volume={10},
  pages={130058--130067},
  year={2022},
  publisher={IEEE}
}

@article{hou2020robust,
  title={Robust reinforcement learning with Wasserstein constraint},
  author={Hou, Linfang and Pang, Liang and Hong, Xin and Lan, Yanyan and Ma, Zhiming and Yin, Dawei},
  journal={arXiv preprint arXiv:2006.00945},
  year={2020}
}
@article{abdullah2019wasserstein,
  title={Wasserstein robust reinforcement learning},
  author={Abdullah, Mohammed Amin and Ren, Hang and Ammar, Haitham Bou and Milenkovic, Vladimir and Luo, Rui and Zhang, Mingtian and Wang, Jun},
  journal={arXiv preprint arXiv:1907.13196},
  year={2019}
}

@article{lecarpentier2019non,
  title={Non-stationary Markov decision processes, a worst-case approach using model-based reinforcement learning},
  author={Lecarpentier, Erwan and Rachelson, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{lecarpentier2019non,
  title={Non-stationary Markov decision processes, a worst-case approach using model-based reinforcement learning},
  author={Lecarpentier, Erwan and Rachelson, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yang2017convex,
  title={A convex optimization approach to distributionally robust Markov decision processes with Wasserstein distance},
  author={Yang, Insoon},
  journal={IEEE control systems letters},
  volume={1},
  number={1},
  pages={164--169},
  year={2017},
  publisher={IEEE}
}

@article{abdullah2019wasserstein,
  title={Wasserstein robust reinforcement learning},
  author={Abdullah, Mohammed Amin and Ren, Hang and Ammar, Haitham Bou and Milenkovic, Vladimir and Luo, Rui and Zhang, Mingtian and Wang, Jun},
  journal={arXiv preprint arXiv:1907.13196},
  year={2019}
}

@article{park2023metra,
  title={Metra: Scalable unsupervised rl with metric-aware abstraction},
  author={Park, Seohong and Rybkin, Oleh and Levine, Sergey},
  journal={arXiv preprint arXiv:2310.08887},
  year={2023}
}

@article{farebrother2023proto,
  title={Proto-value networks: Scaling representation learning with auxiliary tasks},
  author={Farebrother, Jesse and Greaves, Joshua and Agarwal, Rishabh and Lan, Charline Le and Goroshin, Ross and Castro, Pablo Samuel and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2304.12567},
  year={2023}
}


@inproceedings{sidford2018variance,
  title={Variance reduced value iteration and faster algorithms for solving markov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={770--787},
  year={2018},
  organization={SIAM}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{
goyal2018transfer,
title={Transfer and Exploration via the Information Bottleneck},
author={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJg8yhAqKm},
}

@inproceedings{
Goyal2020The,
title={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},
author={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hye1kTVFDS}
}

@inproceedings{alemi2016deep,
title	= {Deep Variational Information Bottleneck},
author	= {Alex Alemi and Ian Fischer and Josh Dillon and Kevin Murphy},
year	= {2017},
URL	= {https://arxiv.org/abs/1612.00410},
booktitle	= {ICLR}
}



@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020}
}

@article{yang2019sample,
  title={Sample-optimal parametric q-learning using linearly additive features},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1902.04779},
  year={2019}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2005.12900},
  year={2020}
}

@article{wang2020randomized,
  title={Randomized linear programming solves the Markov decision problem in nearly linear (sometimes sublinear) time},
  author={Wang, Mengdi},
  journal={Mathematics of Operations Research},
  volume={45},
  number={2},
  pages={517--546},
  year={2020},
  publisher={INFORMS}
}

@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@inproceedings{kearns1999finite,
  title={Finite-sample convergence rates for Q-learning and indirect algorithms},
  author={Kearns, Michael J and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={996--1002},
  year={1999}
}

@inproceedings{yang2020harnessing,
  title={Harnessing Structures for Value-Based Planning and Reinforcement Learning},
  author={Yuzhe Yang and Guo Zhang and Zhi Xu and Dina Katabi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=rklHqRVKvH}
}

@inproceedings{dubeyICLRW18human,
    Author = {Dubey, Rachit and Agrawal, Pulkit
    and Pathak, Deepak and Griffiths, Thomas L.
    and Efros, Alexei A.},
    Title = {Investigating Human Priors for
    Playing Video Games},
    Booktitle = {ICML},
    Year = {2018},
}


@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{pananjady2020instance,
  title={Instance-dependent â„“âˆž-bounds for policy evaluation in tabular reinforcement learning},
  author={Pananjady, Ashwin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  year={2020},
  publisher={IEEE}
}

@article{wainwright2019variance,
  title={Variance-reduced $ Q $-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}


@inproceedings{
Wang2020Exploring,
title={Exploring Model-based Planning with Policy Networks},
author={Tingwu Wang and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1exf64KwH}
}

@article{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{brandfonbrener2021offline,
  title={Offline rl without off-policy evaluation},
  author={Brandfonbrener, David and Whitney, Will and Ranganath, Rajesh and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4933--4946},
  year={2021}
}

@article{fang2024diffusion,
  title={Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning},
  author={Fang, Linjiajie and Liu, Ruoxue and Zhang, Jing and Wang, Wenjia and Jing, Bing-Yi},
  journal={arXiv preprint arXiv:2405.20555},
  year={2024}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}

@article{even2003learning,
  title={Learning rates for Q-learning},
  author={Even-Dar, Eyal and Mansour, Yishay},
  journal={Journal of machine learning Research},
  volume={5},
  number={Dec},
  pages={1--25},
  year={2003}
}

@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018}
}

@inproceedings{forejt2011automated,
  title={Automated verification techniques for probabilistic systems},
  author={Forejt, Vojt{\v{e}}ch and Kwiatkowska, Marta and Norman, Gethin and Parker, David},
  booktitle={International School on Formal Methods for the Design of Computer, Communication and Software Systems},
  pages={53--113},
  year={2011},
  organization={Springer}
}

@inproceedings{
liu2018competitive,
title={Competitive experience replay},
author={Hao Liu and Alexander Trott and Richard Socher and Caiming Xiong},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Sklsm20ctX},
}

@incollection{ng2006autonomous,
  title={Autonomous inverted helicopter flight via reinforcement learning},
  author={Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
  booktitle={Experimental robotics IX},
  pages={363--372},
  year={2006},
  publisher={Springer}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{levine2018learning,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={421--436},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{tian2019elf,
  title={Elf opengo: An analysis and open reimplementation of alphazero},
  author={Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1902.04522},
  year={2019}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@article{li2018deep,
  title={Deep reinforcement learning},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1810.06339},
  year={2018}
}

@article{guestrin2003efficient,
  title={Efficient solution algorithms for factored {MDPs}},
  author={Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
  journal={Journal of Artificial Intelligence Research},
  volume={19},
  pages={399--468},
  year={2003}
}

@article{boutilier2000stochastic,
  title={Stochastic dynamic programming with factored representations},
  author={Boutilier, Craig and Dearden, Richard and Goldszmidt, Mois{\'e}s},
  journal={Artificial intelligence},
  volume={121},
  number={1-2},
  pages={49--107},
  year={2000},
  publisher={Elsevier}
}

%%% Q learning convergence paper

@inproceedings{bertsekas1995neuro,
  title={Neuro-dynamic programming: an overview},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  booktitle={Proceedings of 1995 34th IEEE conference on decision and control},
  volume={1},
  pages={560--564},
  year={1995},
  organization={IEEE}
}

@article{tsitsiklis1994asynchronous,
  title={Asynchronous stochastic approximation and Q-learning},
  author={Tsitsiklis, John N},
  journal={Machine learning},
  volume={16},
  number={3},
  pages={185--202},
  year={1994},
  publisher={Springer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{littman1996generalized,
  title={A generalized reinforcement-learning model: Convergence and applications},
  author={Littman, Michael L and Szepesv{\'a}ri, Csaba},
  booktitle={ICML},
  volume={96},
  pages={310--318},
  year={1996},
  organization={Citeseer}
}

@article{jaakkola1994convergence,
  title={On the convergence of stochastic iterative dynamic programming algorithms},
  author={Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
  journal={Neural computation},
  volume={6},
  number={6},
  pages={1185--1201},
  year={1994},
  publisher={MIT Press}
}

@article{borkar2000ode,
  title={The ODE method for convergence of stochastic approximation and reinforcement learning},
  author={Borkar, Vivek S and Meyn, Sean P},
  journal={SIAM Journal on Control and Optimization},
  volume={38},
  number={2},
  pages={447--469},
  year={2000},
  publisher={SIAM}
}

%%% Q learning convergence paper

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{liu2020learning,
  title={Learning Abstract Models for Strategic Exploration and Fast Reward Transfer},
  author={Liu, Evan Zheran and Keramati, Ramtin and Seshadri, Sudarshan and Guu, Kelvin and Pasupat, Panupong and Brunskill, Emma and Liang, Percy},
  journal={arXiv preprint arXiv:2007.05896},
  year={2020}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for {OpenAI Gym}},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{chi2023diffusion,
  title={Diffusion policy: Visuomotor policy learning via action diffusion},
  author={Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  journal={The International Journal of Robotics Research},
  pages={02783649241273668},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{hilton2023kl,
  author = {Jacob Hilton},
  title = {KL divergence of max-of-n},
  year = {2023},
  url = {https://www.jacobh.co.uk/bon_kl.pdf}
}

@misc{Brockman2016OpenAI,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {{OpenAI Gym}},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{
gleave2021quantifying,
title={Quantifying Differences in Reward Functions},
author={Adam Gleave and Michael D Dennis and Shane Legg and Stuart Russell and Jan Leike},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LwEQnp6CYev}
}

@article{kearns2002sparse,
  title={A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
  author={Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={193--208},
  year={2002},
  publisher={Springer}
}

@article{boutilier1999decision,
  title={Decision-theoretic planning: Structural assumptions and computational leverage},
  author={Boutilier, Craig and Dean, Thomas and Hanks, Steve},
  journal={Journal of Artificial Intelligence Research},
  volume={11},
  pages={1--94},
  year={1999}
}


@book{powell2007approximate,
  title={Approximate Dynamic Programming: Solving the curses of dimensionality},
  author={Powell, Warren B},
  volume={703},
  year={2007},
  publisher={John Wiley \& Sons}
}

@article{rintanen2001overview,
  title={An overview of recent algorithms for AI planning},
  author={Rintanen, Jussi and Hoffmann, J{\"o}rg},
  journal={KI},
  volume={15},
  number={2},
  pages={5--11},
  year={2001}
}


%%%%%
% Books for MDP
%%%%%

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@book{bertsekas2019reinforcement,
  title={Reinforcement Learning and Optimal Control},
  author={Bertsekas, Dimitri P},
  year = {2019},
  publisher = {Athena Scientific}
}

@book{bertsekas1995dynamic,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, Dimitri P},
  year={1995},
  publisher={Athena Scientific}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

%%%%%
% Hierarchical Robotics Planning
%%%%%

@inproceedings{kaelbling2011hierarchical,
  title={Hierarchical task and motion planning in the now},
  author={Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={1470--1477},
  year={2011},
  organization={IEEE}
}

@article{silver2021learning,
  title={Learning Symbolic Operators for Task and Motion Planning},
  author={Silver, Tom and Chitnis, Rohan and Tenenbaum, Joshua and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
  journal={arXiv preprint arXiv:2103.00589},
  year={2021}
}

@inproceedings{singh2009rewards,
  title={Where do rewards come from},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  booktitle={Proceedings of the annual conference of the cognitive science society},
  pages={2601--2606},
  year={2009},
  organization={Cognitive Science Society}
}

@article{singh2010intrinsically,
  title={Intrinsically motivated reinforcement learning: An evolutionary perspective},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={2},
  pages={70--82},
  year={2010},
  publisher={IEEE}
}


%%%%%%
% Methods that study the hierarchical structures with subgoals
%%%%%%

@article{wen2020efficiency,
  title={On Efficiency in Hierarchical Reinforcement Learning},
  author={Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{kaelbling1993hierarchical,
  title={Hierarchical learning in stochastic domains: Preliminary results},
  author={Kaelbling, Leslie Pack},
  booktitle={Proceedings of the tenth international conference on machine learning},
  volume={951},
  pages={167--173},
  year={1993}
}

@inproceedings{sutton1998intra,
  title={Intra-Option Learning about Temporally Abstract Actions.},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder P},
  booktitle={ICML},
  volume={98},
  pages={556--564},
  year={1998}
}

@inproceedings{diuk2008object,
  title={An object-oriented representation for efficient reinforcement learning},
  author={Diuk, Carlos and Cohen, Andre and Littman, Michael L},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={240--247},
  year={2008}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e}, Hal},
  booktitle={International Conference on Machine Learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}

@article{barto2003recent,
  title={Recent advances in hierarchical reinforcement learning},
  author={Barto, Andrew G and Mahadevan, Sridhar},
  journal={Discrete event dynamic systems},
  volume={13},
  number={1},
  pages={41--77},
  year={2003},
  publisher={Springer}
}

@article{parr1998reinforcement,
  title={Reinforcement learning with hierarchies of machines},
  author={Parr, Ronald and Russell, Stuart},
  journal={Advances in neural information processing systems},
  pages={1043--1049},
  year={1998},
  publisher={MORGAN KAUFMANN PUBLISHERS}
}

@book{parr1998hierarchical,
  title={Hierarchical control and learning for Markov decision processes},
  author={Parr, Ronald Edward},
  year={1998},
  publisher={University of California, Berkeley Berkeley, CA}
}

%% Subgoals
@inproceedings{dean1995decomposition,
  title={Decomposition techniques for planning in stochastic domains},
  author={Dean, Thomas and Lin, Shieu-Hong},
  booktitle={IJCAI},
  volume={2},
  pages={3},
  year={1995},
  organization={Citeseer}
}

@article{singh1998dynamically,
  title={How to dynamically merge Markov decision processes},
  author={Singh, Satinder and Cohn, David and others},
  journal={Advances in neural information processing systems},
  number={10},
  pages={1057--1063},
  year={1998},
  publisher={Citeseer}
}

@inproceedings{meuleau1998solving,
  title={Solving very large weakly coupled Markov decision processes},
  author={Meuleau, Nicolas and Hauskrecht, Milos and Kim, Kee-Eung and Peshkin, Leonid and Kaelbling, Leslie Pack and Dean, Thomas L and Boutilier, Craig},
  booktitle={AAAI/IAAI},
  pages={165--172},
  year={1998}
}

@article{mann2015approximate,
  title={Approximate value iteration with temporally extended actions},
  author={Mann, Timothy A and Mannor, Shie and Precup, Doina},
  journal={Journal of Artificial Intelligence Research},
  volume={53},
  pages={375--438},
  year={2015}
}

@inproceedings{stolle2002learning,
  title={Learning options in reinforcement learning},
  author={Stolle, Martin and Precup, Doina},
  booktitle={International Symposium on abstraction, reformulation, and approximation},
  pages={212--223},
  year={2002},
  organization={Springer}
}

@inproceedings{simsek2008skill,
  title={Skill characterization based on betweenness},
  author={Simsek, Ozgur and Barreto, Andre S},
  booktitle={Advances in neural information processing systems},
  pages={1497--1504},
  year={2008}
}

@article{solway2014optimal,
  title={Optimal behavioral hierarchy},
  author={Solway, Alec and Diuk, Carlos and C{\'o}rdova, Natalia and Yee, Debbie and Barto, Andrew G and Niv, Yael and Botvinick, Matthew M},
  journal={PLOS Comput Biol},
  volume={10},
  number={8},
  pages={e1003779},
  year={2014},
  publisher={Public Library of Science}
}

@inproceedings{jiang2016structural,
  title={On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={IJCAI},
  pages={1640--1647},
  year={2016}
}

@inproceedings{bakker2004hierarchical,
  title={Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization},
  author={Bakker, Bram and Schmidhuber, J{\"u}rgen and others},
  booktitle={Proc. of the 8-th Conf. on Intelligent Autonomous Systems},
  pages={438--445},
  year={2004},
  organization={Citeseer}
}

@article{mcgovern2001automatic,
  title={Automatic discovery of subgoals in reinforcement learning using diverse density},
  author={McGovern, Amy and Barto, Andrew G},
  year={2001}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

%% Robotics planning

@inproceedings{gopalan2017planning,
  title={Planning with abstract Markov decision processes},
  author={Gopalan, Nakul and Littman, Michael and MacGlashan, James and Squire, Shawn and Tellex, Stefanie and Winder, John and Wong, Lawson and others},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={27},
  number={1},
  year={2017}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

@book{lavalle2006planning,
  title={Planning algorithms},
  author={LaValle, Steven M},
  year={2006},
  publisher={Cambridge university press}
}

@book{siciliano2010robotics,
  title={Robotics: modelling, planning and control},
  author={Siciliano, Bruno and Sciavicco, Lorenzo and Villani, Luigi and Oriolo, Giuseppe},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@book{russell2016artificial,
  title={Artificial Intelligence: a modern approach},
  author={Russell, Stuart J. and Norvig, Peter},
  edition={3},
  year={2009},
  publisher={Pearson}
}

%% Planning & Subgoals & MDP

@article{xu2020hierarchial,
  title={Hierarchial Reinforcement Learning in StarCraft II with Human Expertise in Subgoals Selection},
  author={Xu, Xinyi and Huang, Tiancheng and Wei, Pengfei and Narayan, Akshay and Leong, Tze-Yun},
  journal={arXiv preprint arXiv:2008.03444},
  year={2020}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

@inproceedings{mcgovern1998hierarchical,
  title={Hierarchical optimal control of MDPs},
  author={McGovern, Amy and Precup, Doina and Ravindran, Balaraman and Singh, Satinder and Sutton, Richard S},
  booktitle={Proceedings of the Tenth Yale Workshop on Adaptive and Learning Systems},
  pages={186--191},
  year={1998}
}

@article{hauskrecht2013hierarchical,
  title={Hierarchical solution of Markov decision processes using macro-actions},
  author={Hauskrecht, Milos and Meuleau, Nicolas and Kaelbling, Leslie Pack and Dean, Thomas L and Boutilier, Craig},
  journal={arXiv preprint arXiv:1301.7381},
  year={2013}
}

@article{precup1998multi,
  title={Multi-time models for temporally abstract planning},
  author={Precup, Doina and Sutton, Richard S and Singh, S},
  journal={Advances in neural information processing systems},
  pages={1050--1056},
  year={1998},
  publisher={MORGAN KAUFMANN PUBLISHERS}
}
%%%%%
%% Carefully designed reward function papers:
%%%%%

@article{oh2015action,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
  journal={arXiv preprint arXiv:1507.08750},
  year={2015}
}

@article{sorg2010reward,
  title={Reward design via online gradient ascent},
  author={Sorg, Jonathan and Lewis, Richard L and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={2190--2198},
  year={2010},
  publisher={Citeseer}
}

@article{guo2014deep,
  title={Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning},
  author={Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard L and Wang, Xiaoshi},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={3338--3346},
  year={2014}
}

@phdthesis{guo2017deep,
  title={Deep learning and reward design for reinforcement learning},
  author={Guo, Xiaoxiao},
  year={2017}
}

@article{rudner2021outcome,
  title={Outcome-Driven Reinforcement Learning via Variational Inference},
  author={Rudner, Tim GJ and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2104.10190},
  year={2021}
}

%(The pacman)
@inproceedings{
Raileanu2020RIDE:,
title={RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments},
author={Roberta Raileanu and Tim RocktÃ¤schel},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkg-TJBFPB}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@misc{OpenAI_dota,
      author = {OpenAI},
      title = {{OpenAI Five}},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}
}

@article{fuchs2021super,
  title={Super-human performance in gran turismo sport using deep reinforcement learning},
  author={Fuchs, Florian and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide and D{\"u}rr, Peter},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={4257--4264},
  year={2021},
  publisher={IEEE}
}

@inproceedings{ye2020towards,
 author = {Ye, Deheng and Chen, Guibin and Zhang, Wen and Chen, Sheng and Yuan, Bo and Liu, Bo and Chen, Jia and Liu, Zhao and Qiu, Fuhao and Yu, Hongsheng and Yin, Yinyuting and Shi, Bei and Wang, Liang and Shi, Tengfei and Fu, Qiang and Yang, Wei and Huang, Lanxiao and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {621--632},
 publisher = {Curran Associates, Inc.},
 title = {Towards Playing Full MOBA Games with Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{vinyals2017starcraft,
  title={Starcraft ii: A new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@inproceedings{racaniere2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdomenech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={5694--5705},
  year={2017}
}

@article{popov2017data,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}

%%%
% Negative result of ``bad'' reward design
%%%

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@inproceedings{VanSeijen2017Hybrid,
 author = {Van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hybrid Reward Architecture for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/1264a061d82a2edae1574b07249800d6-Paper.pdf},
 volume = {30},
 year = {2017}
}



%%%
% Deep Learning
%%%

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  number={2},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{Krakovsky2016Reinforcement,
author = {Krakovsky, Marina},
title = {Reinforcement Renaissance},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/2949662},
doi = {10.1145/2949662},
abstract = {The power of deep neural networks has sparked renewed interest in reinforcement learning, with applications to games, robotics, and beyond.},
journal = {Commun. ACM},
month = jul,
pages = {12â€“14},
numpages = {3}
}

%% Mujoco

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE}
}

@inproceedings{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2015},
      booktitle={International Conference on Learning Representations},
}

@misc{tieleman2012rmsprop,
  title={Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

%% Citing for JAIR
@article{kolter2011fixed,
  title={The fixed points of off-policy TD},
  author={Kolter, J},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={2169--2177},
  year={2011}
}

@article{doroudi2019s,
  title={Whereâ€™s the reward?},
  author={Doroudi, Shayan and Aleven, Vincent and Brunskill, Emma},
  journal={International Journal of Artificial Intelligence in Education},
  volume={29},
  number={4},
  pages={568--620},
  year={2019},
  publisher={Springer}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}

%%% Reward Design HCI and Robotics
@article{he2021assisted,
  title={Assisted Robust Reward Design},
  author={He, Jerry Zhi-Yang and Dragan, Anca D},
  journal={arXiv preprint arXiv:2111.09884},
  year={2021}
}

%%% Goal Conditioned and Reward Design

@article{nasiriany2019planning,
  title={Planning with goal-conditioned policies},
  author={Nasiriany, Soroush and Pong, Vitchyr H and Lin, Steven and Levine, Sergey},
  journal={arXiv preprint arXiv:1911.08453},
  year={2019}
}

@inproceedings{schmidhuber1991curious,
  title={Curious model-building control systems},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. international joint conference on neural networks},
  pages={1458--1463},
  year={1991}
}

@inproceedings{portelas2020teacher,
  title={Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments},
  author={Portelas, R{\'e}my and Colas, C{\'e}dric and Hofmann, Katja and Oudeyer, Pierre-Yves},
  booktitle={Conference on Robot Learning},
  pages={835--853},
  year={2020},
  organization={PMLR}
}

@article{nair2018visual,
  title={Visual reinforcement learning with imagined goals},
  author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{eysenbach2020rewriting,
  title={Rewriting history with inverse rl: Hindsight inference for policy improvement},
  author={Eysenbach, Ben and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14783--14795},
  year={2020}
}

@article{mendonca2021discovering,
  title={Discovering and achieving goals via world models},
  author={Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International conference on machine learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{hafner2020mastering,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2020}
}

@article{peng2018variational,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@inproceedings{tang2021hindsight,
  title={Hindsight expectation maximization for goal-conditioned reinforcement learning},
  author={Tang, Yunhao and Kucukelbir, Alp},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2863--2871},
  year={2021},
  organization={PMLR}
}

@article{borsa2018universal,
  title={Universal successor features approximators},
  author={Borsa, Diana and Barreto, Andr{\'e} and Quan, John and Mankowitz, Daniel and Munos, R{\'e}mi and Van Hasselt, Hado and Silver, David and Schaul, Tom},
  journal={arXiv preprint arXiv:1812.07626},
  year={2018}
}

@article{ghosh2019learning,
  title={Learning to reach goals via iterated supervised learning},
  author={Ghosh, Dibya and Gupta, Abhishek and Reddy, Ashwin and Fu, Justin and Devin, Coline and Eysenbach, Benjamin and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.06088},
  year={2019}
}

@article{eysenbach2020c,
  title={C-learning: Learning to achieve goals via recursive classification},
  author={Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
  journal={arXiv preprint arXiv:2011.08909},
  year={2020}
}

@article{rudner2021outcome,
  title={Outcome-Driven Reinforcement Learning via Variational Inference},
  author={Rudner, Tim GJ and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2104.10190},
  year={2021}
}

@article{fu2018variational,
  title={Variational inverse control with events: A general framework for data-driven reward definition},
  author={Fu, Justin and Singh, Avi and Ghosh, Dibya and Yang, Larry and Levine, Sergey},
  journal={arXiv preprint arXiv:1805.11686},
  year={2018}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@article{eysenbach2021replacing,
  title={Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification},
  author={Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2103.12656},
  year={2021}
}

@article{chebotar2021actionable,
  title={Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills},
  author={Chebotar, Yevgen and Hausman, Karol and Lu, Yao and Xiao, Ted and Kalashnikov, Dmitry and Varley, Jake and Irpan, Alex and Eysenbach, Benjamin and Julian, Ryan and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2104.07749},
  year={2021}
}

@article{khazatsky2021can,
  title={What Can I Do Here? Learning New Skills by Imagining Visual Affordances},
  author={Khazatsky, Alexander and Nair, Ashvin and Jing, Daniel and Levine, Sergey},
  journal={arXiv preprint arXiv:2106.00671},
  year={2021}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

@inproceedings{sutton2011horde,
  title={Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
  author={Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={761--768},
  year={2011}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{pong2018temporal,
  title={Temporal difference models: Model-free deep {RL} for model-based control},
  author={Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.09081},
  year={2018}
}


@article{kadian2020sim2real,
  title={Sim2Real predictivity: Does evaluation in simulation predict real-world performance?},
  author={Kadian, Abhishek and Truong, Joanne and Gokaslan, Aaron and Clegg, Alexander and Wijmans, Erik and Lee, Stefan and Savva, Manolis and Chernova, Sonia and Batra, Dhruv},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={4},
  pages={6670--6677},
  year={2020},
  publisher={IEEE}
}

@inproceedings{fujita2020distributed,
  title={Distributed Reinforcement Learning of Targeted Grasping with Active Vision for Mobile Manipulators},
  author={Fujita, Yasuhiro and Uenishi, Kota and Ummadisingu, Avinash and Nagarajan, Prabhat and Masuda, Shimpei and Castro, Mario Ynocente},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9712--9719},
  year={2020},
  organization={IEEE}
}

@article{ratner2018simplifying,
  title={Simplifying reward design through divide-and-conquer},
  author={Ratner, Ellis and Hadfield-Menell, Dylan and Dragan, Anca D},
  journal={arXiv preprint arXiv:1806.02501},
  year={2018}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={PMLR}
}

@article{parker2022evolving,
  title={Evolving Curricula with Regret-Based Environment Design},
  author={Parker-Holder, Jack and Jiang, Minqi and Dennis, Michael and Samvelyan, Mikayel and Foerster, Jakob and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2203.01302},
  year={2022}
}

@article{fang2020adaptive,
  title={Adaptive procedural task generation for hard-exploration problems},
  author={Fang, Kuan and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2007.00350},
  year={2020}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@article{zhang2021c,
  title={C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks},
  author={Zhang, Tianjun and Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2110.12080},
  year={2021}
}

@inproceedings{pitis2020maximum,
  title={Maximum entropy gain exploration for long horizon multi-goal reinforcement learning},
  author={Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly and Ba, Jimmy},
  booktitle={International Conference on Machine Learning},
  pages={7750--7761},
  year={2020},
  organization={PMLR}
}

@article{dennis2020emergent,
  title={Emergent complexity and zero-shot transfer via unsupervised environment design},
  author={Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13049--13061},
  year={2020}
}

@article{matiisen2019teacher,
  title={Teacher--student curriculum learning},
  author={Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={9},
  pages={3732--3740},
  year={2019},
  publisher={IEEE}
}


@article{warde2018unsupervised,
  title={Unsupervised control through non-parametric discriminative rewards},
  author={Warde-Farley, David and Van de Wiele, Tom and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1811.11359},
  year={2018}
}

@inproceedings{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle={Conference on robot learning},
  pages={482--495},
  year={2017},
  organization={PMLR}
}

@inproceedings{
fan2018learning,
title={Learning to Teach},
author={Yang Fan and Fei Tian and Tao Qin and Xiang-Yang Li and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJewuJWCZ},
}

@article{wu2018learning,
  title={Learning to teach with dynamic loss functions},
  author={Wu, Lijun and Tian, Fei and Xia, Yingce and Fan, Yang and Qin, Tao and Jian-Huang, Lai and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{weinshall2018curriculum,
  title={Curriculum learning by transfer learning: Theory and experiments with deep networks},
  author={Weinshall, Daphna and Cohen, Gad and Amir, Dan},
  booktitle={International Conference on Machine Learning},
  pages={5238--5246},
  year={2018},
  organization={PMLR}
}

@inproceedings{such2020generative,
  title={Generative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data},
  author={Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth and Clune, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={9206--9216},
  year={2020},
  organization={PMLR}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{ivanovic2019barc,
  title={{BARC}: Backward reachability curriculum for robotic reinforcement learning},
  author={Ivanovic, Boris and Harrison, James and Sharma, Apoorva and Chen, Mo and Pavone, Marco},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={15--21},
  year={2019},
  organization={IEEE}
}

@article{kim2018screenernet,
  title={Screenernet: Learning self-paced curriculum for deep neural networks},
  author={Kim, Tae-Hoon and Choi, Jonghyun},
  journal={arXiv preprint arXiv:1801.00904},
  year={2018}
}

@inproceedings{omidshafiei2019learning,
  title={Learning to teach in cooperative multiagent reinforcement learning},
  author={Omidshafiei, Shayegan and Kim, Dong-Ki and Liu, Miao and Tesauro, Gerald and Riemer, Matthew and Amato, Christopher and Campbell, Murray and How, Jonathan P},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33 issue 01},
  pages={6128--6136},
  year={2019}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}

@article{ghosh2018learning,
  title={Learning actionable representations with goal-conditioned policies},
  author={Ghosh, Dibya and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:1811.07819},
  year={2018}
}

@article{sukhbaatar2018learning,
  title={Learning goal embeddings via self-play for hierarchical reinforcement learning},
  author={Sukhbaatar, Sainbayar and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1811.09083},
  year={2018}
}

@inproceedings{
ghosh2021learning,
title={Learning to Reach Goals via Iterated Supervised Learning},
author={Dibya Ghosh and Abhishek Gupta and Ashwin Reddy and Justin Fu and Coline Manon Devin and Benjamin Eysenbach and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rALA0Xo6yNJ}
}

@article{sukhbaatar2017intrinsic,
  title={Intrinsic motivation and automatic curricula via asymmetric self-play},
  author={Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1703.05407},
  year={2017}
}

@inproceedings{florensa2018automatic,
  title={Automatic goal generation for reinforcement learning agents},
  author={Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1515--1528},
  year={2018},
  organization={PMLR}
}

@article{openai2021asymmetric,
  title={Asymmetric self-play for automatic goal discovery in robotic manipulation},
  author={OpenAI, OpenAI and Plappert, Matthias and Sampedro, Raul and Xu, Tao and Akkaya, Ilge and Kosaraju, Vineet and Welinder, Peter and D'Sa, Ruben and Petron, Arthur and Pinto, Henrique P d O and others},
  journal={arXiv preprint arXiv:2101.04882},
  year={2021}
}

@inproceedings{
klink2022boosted,
title={Boosted Curriculum Reinforcement Learning},
author={Pascal Klink and Carlo D'Eramo and Jan Peters and Joni Pajarinen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=anbBFlX1tJ1}
}

@article{ecoffet2019go,
  title={{Go-Explore}: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@article{hartikainen2019dynamical,
  title={Dynamical distance learning for semi-supervised and unsupervised skill discovery},
  author={Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey},
  journal={arXiv preprint arXiv:1907.08225},
  year={2019}
}

@article{ren2019exploration,
  title={Exploration via hindsight goal generation},
  author={Ren, Zhizhou and Dong, Kefan and Zhou, Yuan and Liu, Qiang and Peng, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{klink2020self,
  title={Self-paced deep reinforcement learning},
  author={Klink, Pascal and D'Eramo, Carlo and Peters, Jan R and Pajarinen, Joni},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9216--9227},
  year={2020}
}

@inproceedings{nair2022learning,
  title={Learning language-conditioned robot behavior from offline data and crowd-sourced annotation},
  author={Nair, Suraj and Mitchell, Eric and Chen, Kevin and Savarese, Silvio and Finn, Chelsea and others},
  booktitle={Conference on Robot Learning},
  pages={1303--1315},
  year={2022},
  organization={PMLR}
}

@article{zhang2020automatic,
  title={Automatic curriculum learning through value disagreement},
  author={Zhang, Yunzhi and Abbeel, Pieter and Pinto, Lerrel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7648--7659},
  year={2020}
}

@InProceedings{pmlr-v37-schaul15,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters Î¸. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}


@article{kalashnikov2021mt,
  title={{MT-opt}: Continuous multi-task robotic reinforcement learning at scale},
  author={Kalashnikov, Dmitry and Varley, Jacob and Chebotar, Yevgen and Swanson, Benjamin and Jonschkowski, Rico and Finn, Chelsea and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:2104.08212},
  year={2021}
}

@inproceedings{yu2020meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}

@inproceedings{nair2020contextual,
  title={Contextual imagined goals for self-supervised robotic learning},
  author={Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={530--539},
  year={2020},
  organization={PMLR}
}

@book{allgower2012numerical,
  title={Numerical continuation methods: an introduction},
  author={Allgower, Eugene L and Georg, Kurt},
  volume={13},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@inproceedings{li2021mural,
  title={MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning},
  author={Li, Kevin and Gupta, Abhishek and Reddy, Ashwin and Pong, Vitchyr H and Zhou, Aurick and Yu, Justin and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={6346--6356},
  year={2021},
  organization={PMLR}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{zhai2022computational,
  title={Computational Benefits of Intermediate Rewards for Goal-Reaching Policy Learning},
  author={Zhai, Yuexiang and Baek, Christina and Zhou, Zhengyuan and Jiao, Jiantao and Ma, Yi},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={847--896},
  year={2022}
}

@article{janner2021offline,
  title={Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@inproceedings{sodhani2021multi,
  title={Multi-task reinforcement learning with context-based representations},
  author={Sodhani, Shagun and Zhang, Amy and Pineau, Joelle},
  booktitle={International Conference on Machine Learning},
  pages={9767--9779},
  year={2021},
  organization={PMLR}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}


@article{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{rechtoptimization,
  title={Optimization for Modern Data Analysis. 2019},
  author={Recht, Benjamin and Wright, Stephen J},
  journal={Preprint available at http://eecs. berkeley. edu/brecht/opt4mlbook}
}


@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@book{haarnoja2018acquiring,
  title={Acquiring diverse robot skills via maximum entropy deep reinforcement learning},
  author={Haarnoja, Tuomas},
  year={2018},
  publisher={University of California, Berkeley}
}

@article{ding2021beyond,
  title={Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods with Entropy Regularization},
  author={Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  journal={arXiv preprint arXiv:2110.10117},
  year={2021}
}

@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  year={2019}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M and Packer, Benjamin and Koller, Daphne},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@inproceedings{murali2018cassl,
  title={Cassl: Curriculum accelerated self-supervised learning},
  author={Murali, Adithyavairavan and Pinto, Lerrel and Gandhi, Dhiraj and Gupta, Abhinav},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6453--6460},
  year={2018},
  organization={IEEE}
}

@article{borsa2016learning,
  title={Learning shared representations in multi-task reinforcement learning},
  author={Borsa, Diana and Graepel, Thore and Shawe-Taylor, John},
  journal={arXiv preprint arXiv:1603.02041},
  year={2016}
}

@article{calandriello2014sparse,
  title={Sparse multi-task reinforcement learning},
  author={Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{maurer2016benefit,
  title={The benefit of multitask representation learning},
  author={Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={81},
  pages={1--32},
  year={2016}
}

@article{wu2019comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
  journal={arXiv preprint arXiv:1901.00596},
  year={2019}
}

@article{selsam2018learning,
  title={Learning a SAT solver from single-bit supervision},
  author={Selsam, Daniel and Lamm, Matthew and B{\"u}nz, Benedikt and Liang, Percy and de Moura, Leonardo and Dill, David L},
  journal={arXiv preprint arXiv:1802.03685},
  year={2018}
}

@article{davis1962machine,
  title={A machine program for theorem-proving},
  author={Davis, Martin and Logemann, George and Loveland, Donald},
  journal={Communications of the ACM},
  volume={5},
  number={7},
  pages={394--397},
  year={1962},
  publisher={ACM New York, NY, USA}
}

@article{bansal2019holist,
  title={HOList: An environment for machine learning of higher-order theorem proving (extended version)},
  author={Bansal, Kshitij and Loos, Sarah M and Rabe, Markus N and Szegedy, Christian and Wilcox, Stewart},
  journal={arXiv preprint arXiv:1904.03241},
  year={2019}
}


@article{wu2018understanding,
  title={Understanding short-horizon bias in stochastic meta-optimization},
  author={Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.02021},
  year={2018}
}

@article{huang2018gamepad,
  title={Gamepad: A learning environment for theorem proving},
  author={Huang, Daniel and Dhariwal, Prafulla and Song, Dawn and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1806.00608},
  year={2018}
}

@article{paliwal2019graph,
  title={Graph Representations for Higher-Order Logic and Theorem Proving},
  author={Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian},
  journal={arXiv preprint arXiv:1905.10006},
  year={2019}
}

@inproceedings{wang2017premise,
  title={Premise selection for theorem proving by deep graph embedding},
  author={Wang, Mingzhe and Tang, Yihe and Wang, Jian and Deng, Jia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2786--2796},
  year={2017}
}

@inproceedings{schulz2013system,
  title={System description: E 1.8},
  author={Schulz, Stephan},
  booktitle={International Conference on Logic for Programming Artificial Intelligence and Reasoning},
  pages={735--743},
  year={2013},
  organization={Springer}
}

@inproceedings{selman1992new,
  title={A New Method for Solving Hard Satisfiability Problems.},
  author={Selman, Bart and Levesque, Hector J and Mitchell, David G and others},
  organization={Citeseer}
}

@inproceedings{nudelman2004understanding,
  title={Understanding random SAT: Beyond the clauses-to-variables ratio},
  author={Nudelman, Eugene and Leyton-Brown, Kevin and Hoos, Holger H and Devkar, Alex and Shoham, Yoav},
  booktitle={International Conference on Principles and Practice of Constraint Programming},
  pages={438--452},
  year={2004},
  organization={Springer}
}


@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{ellis2018learning,
  title={Learning libraries of subroutines for neurally--guided bayesian program induction},
  author={Ellis, Kevin and Morales, Lucas and Sabl{\'e}-Meyer, Mathias and Solar-Lezama, Armando and Tenenbaum, Josh},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7805--7815},
  year={2018}
}


@article{silver2019few,
  title={Few-Shot Bayesian Imitation Learning with Logic over Programs},
  author={Silver, Tom and Allen, Kelsey R and Lew, Alex K and Kaelbling, Leslie Pack and Tenenbaum, Josh},
  journal={arXiv preprint arXiv:1904.06317},
  year={2019}
}

@article{cropper2019playgol,
  title={Playgol: learning programs through play},
  author={Cropper, Andrew},
  journal={arXiv preprint arXiv:1904.08993},
  year={2019}
}

@inproceedings{ritchie2016neurally,
  title={Neurally-guided procedural models: Amortized inference for procedural graphics programs using neural networks},
  author={Ritchie, Daniel and Thomas, Anna and Hanrahan, Pat and Goodman, Noah},
  booktitle={Advances in neural information processing systems},
  pages={622--630},
  year={2016}
}

@article{shin2019program,
  title={Program Synthesis and Semantic Parsing with Learned Code Idioms},
  author={Shin, Richard and Allamanis, Miltiadis and Brockschmidt, Marc and Polozov, Oleksandr},
  journal={arXiv preprint arXiv:1906.10816},
  year={2019}
}


@article{yang2019learning,
  title={Learning to Prove Theorems via Interacting with Proof Assistants},
  author={Yang, Kaiyu and Deng, Jia},
  journal={arXiv preprint arXiv:1905.09381},
  year={2019}
}

@inproceedings{
anonymous2020towards,
title={Towards Finding Longer Proofs},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkeh21BKPH},
note={under review}
}

@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}

@inproceedings{
anonymous2020learning,
title={Learning to Prove Theorems by Learning to Generate Theorems},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJxiqxSYPB},
note={under review}
}

@inproceedings{
anonymous2020adaptive,
title={{\{}ADAPTIVE{\}} {\{}GENERATION{\}} {\{}OF{\}} {\{}PROGRAMMING{\}} {\{}PUZZLES{\}}},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJeRveHKDH},
note={under review}
}

@inproceedings{
anonymous2020what,
title={What Can Neural Networks Reason About?},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJxbJeHFPS},
note={under review}
}

@inproceedings{
anonymous2020synthesizing,
title={Synthesizing Programmatic Policies that Inductively Generalize},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1l8oANFDH},
note={under review}
}

@inproceedings{
anonymous2020neuralguided,
title={Neural-Guided Symbolic Regression with Asymptotic Constraints},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1gTAp4FDB},
note={under review}
}

@article{chang2018automatically,
  title={Automatically composing representation transformations as a means for generalization},
  author={Chang, Michael B and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1807.04640},
  year={2018}
}


@incollection{NIPS2019_9241,
title = {G2SAT: Learning to Generate SAT Formulas},
author = {You, Jiaxuan and Wu, Haoze and Barrett, Clark and Ramanujan, Raghuram and Leskovec, Jure},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {10552--10563},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9241-g2sat-learning-to-generate-sat-formulas.pdf}
}

@inproceedings{burnett2019building,
  title={Building a winning self-driving car in six months},
  author={Burnett, Keenan and Schimpe, Andreas and Samavi, Sepehr and Gridseth, Mona and Liu, Chengzhi Winston and Li, Qiyang and Kroeze, Zachary and Schoellig, Angela P},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={9583--9589},
  year={2019},
  organization={IEEE}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017}
}

@article{mohamed2019monte,
  title={Monte carlo gradient estimation in machine learning},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={arXiv preprint arXiv:1906.10652},
  year={2019}
}

@inproceedings{een2003extensible,
    title={An extensible SAT-solver},
    author={E{\'e}n, Niklas and S{\"o}rensson, Niklas},
    booktitle={International conference on theory and applications of satisfiability testing},
    pages={502--518},
    year={2003},
    organization={Springer}
}
@article{selman1996generating,
  title={Generating hard satisfiability problems},
  author={Selman, Bart and Mitchell, David G and Levesque, Hector J},
  journal={Artificial intelligence},
  volume={81},
  number={1-2},
  pages={17--29},
  year={1996},
  publisher={Elsevier}
}

@article{cheeseman1991really,
  title={Where really hard problems are},
  author={Cheeseman, Peter and Kanefsky, Bob and Taylor, William M.},
  journal={Proc. 12th IJCAI, 1991},
  year={1991}
}
@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}


@inproceedings{kingma2015variational,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Durk P and Salimans, Tim and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2575--2583},
  year={2015}
}

@article{wen2018flipout,
  title={Flipout: Efficient pseudo-independent weight perturbations on mini-batches},
  author={Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.04386},
  year={2018}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}

@inproceedings{cohn1995active,
  title={Active learning with statistical models},
  author={Cohn, David A and Ghahramani, Zoubin and Jordan, Michael I},
  booktitle={Advances in neural information processing systems},
  pages={705--712},
  year={1995}
}

@article{mackay1992information,
  title={Information-based objective functions for active data selection},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={4},
  pages={590--604},
  year={1992},
  publisher={MIT Press}
}


@article{pierrot2019learning,
  title={Learning Compositional Neural Programs with Recursive Tree Search and Planning},
  author={Pierrot, Thomas and Ligner, Guillaume and Reed, Scott and Sigaud, Olivier and Perrin, Nicolas and Laterre, Alexandre and Kas, David and Beguir, Karim and de Freitas, Nando},
  journal={arXiv preprint arXiv:1905.12941},
  year={2019}
}

@incollection{NIPS2019_8724,
title = {Adaptive Auxiliary Task Weighting for Reinforcement Learning},
author = {Lin, Xingyu and Baweja, Harjatin and Kantor, George and Held, David},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4773--4784},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning.pdf}
}


@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in neural information processing systems},
  pages={2348--2356},
  year={2011}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  publisher={Pergamon}
}

@inproceedings{you2019g2sat,
  title={G2SAT: Learning to Generate SAT Formulas},
  author={You, Jiaxuan and Wu, Haoze and Barrett, Clark and Ramanujan, Raghuram and Leskovec, Jure},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10552--10563},
  year={2019}
}


@inproceedings{spitkovsky2010baby,
  title={From baby steps to leapfrog: How less is more in unsupervised dependency parsing},
  author={Spitkovsky, Valentin I and Alshawi, Hiyan and Jurafsky, Daniel},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={751--759},
  year={2010},
  organization={Association for Computational Linguistics}
}

@article{cai2017making,
  title={Making neural programming architectures generalize via recursion},
  author={Cai, Jonathon and Shin, Richard and Song, Dawn},
  journal={arXiv preprint arXiv:1704.06611},
  year={2017}
}

@article{reed2015neural,
  title={Neural programmer-interpreters},
  author={Reed, Scott and De Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06279},
  year={2015}
}

@misc{jaxrl,
  author = {Kostrikov, Ilya},
  doi = {10.5281/zenodo.5535154},
  month = {10},
  title = {{JAXRL: Implementations of Reinforcement Learning algorithms in JAX}},
  url = {https://github.com/ikostrikov/jaxrl},
  year = {2021}
}

@inproceedings{houthooft2016vime,
  title={Vime: Variational information maximizing exploration},
  author={Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1109--1117},
  year={2016}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@article{wu2020curricula,
  title={When do curricula work?},
  author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2012.03107},
  year={2020}
}

@article{cobbe2019procgen,
  title={Leveraging Procedural Generation to Benchmark Reinforcement Learning},
  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  journal={arXiv preprint arXiv:1912.01588},
  year={2019}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={arXiv preprint arXiv:2003.04960},
  year={2020}
}


@inproceedings{tanaka2003multitask,
  title={Multitask reinforcement learning on the distribution of {MDPs}},
  author={Tanaka, Fumihide and Yamamura, Masayuki},
  booktitle={Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No. 03EX694)},
  volume={3},
  pages={1108--1113},
  year={2003},
  organization={IEEE}
}

@article{rusu2015policy,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}

@article{rajeswaran2016epopt,
  title={Epopt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.01283},
  year={2016}
}

@inproceedings{
D'Eramo2020Sharing,
title={Sharing Knowledge in Multi-Task Deep Reinforcement Learning},
author={Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgpv2VFvr}
}

@article{yu2020gradient,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5824--5836},
  year={2020}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{bhandari2019global,
  title={Global optimality guarantees for policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@article{cen2021fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  year={2021},
  publisher={INFORMS}
}

@inproceedings{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle={International Conference on Machine Learning},
  pages={1467--1476},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2021softmax,
  title={Softmax policy gradient methods can take exponential time to converge},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  booktitle={Conference on Learning Theory},
  pages={3107--3110},
  year={2021},
  organization={PMLR}
}

@article{zhang2020global,
  title={Global convergence of policy gradient methods to (almost) locally optimal policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Basar, Tamer},
  journal={SIAM Journal on Control and Optimization},
  volume={58},
  number={6},
  pages={3586--3612},
  year={2020},
  publisher={SIAM}
}

@article{zhang2020sample,
  title={Sample efficient reinforcement learning with REINFORCE},
  author={Zhang, Junzi and Kim, Jongho and Oâ€™Donoghue, Brendan and Boyd, Stephen},
  journal={arXiv preprint arXiv:2010.11364},
  pages={97},
  year={2020}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{tang2017exploration,
  title={\# Exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, OpenAI Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2753--2762},
  year={2017}
}

@article{stadie2015incentivizing,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}

%% Contextual MDPs

@article{belogolovsky2021inverse,
  title={Inverse reinforcement learning in contextual {MDPs}},
  author={Belogolovsky, Stav and Korsunsky, Philip and Mannor, Shie and Tessler, Chen and Zahavy, Tom},
  journal={Machine Learning},
  volume={110},
  number={9},
  pages={2295--2334},
  year={2021},
  publisher={Springer}
}

@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low {Bellman} rank are {PAC}-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@inproceedings{modi2018markov,
  title={Markov decision processes with continuous side information},
  author={Modi, Aditya and Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
  booktitle={Algorithmic Learning Theory},
  pages={597--618},
  year={2018},
  organization={PMLR}
}

@article{abbasi2014online,
  title={Online learning in {MDPs} with side information},
  author={Abbasi-Yadkori, Yasin and Neu, Gergely},
  journal={arXiv preprint arXiv:1406.6812},
  year={2014}
}

@inproceedings{modi2020sample,
  title={Sample complexity of reinforcement learning using linearly combined model ensembles},
  author={Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2010--2020},
  year={2020},
  organization={PMLR}
}

@inproceedings{misra2020kinematic,
  title={Kinematic state abstraction and provably efficient rich-observation reinforcement learning},
  author={Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
  booktitle={International conference on machine learning},
  pages={6961--6971},
  year={2020},
  organization={PMLR}
}

@inproceedings{sun2019model,
  title={Model-based {RL} in contextual decision processes: {PAC} bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on learning theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}


@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{kulkarni2016deep,
  title={Deep successor reinforcement learning},
  author={Kulkarni, Tejas D and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J},
  journal={arXiv preprint arXiv:1606.02396},
  year={2016}
}

@article{siriwardhana2019vusfa,
  title={{VUSFA}: Variational universal successor features approximator to improve transfer {DRL} for target driven visual navigation},
  author={Siriwardhana, Shamane and Weerasakera, Rivindu and Matthies, Denys JC and Nanayakkara, Suranga},
  journal={arXiv preprint arXiv:1908.06376},
  year={2019}
}


@inproceedings{yuan2022general,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Yuan, Rui and Gower, Robert M and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3332--3380},
  year={2022},
  organization={PMLR}
}

@article{jin2018q,
  title={Is {Q-learning} provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@article{li2021breaking,
  title={Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{agarwal2020pc,
  title={{PC-PG}: Policy cover directed exploration for provable policy gradient learning},
  author={Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13399--13412},
  year={2020}
}

@book{aastrom2013adaptive,
  title={Adaptive control},
  author={{\AA}str{\"o}m, Karl J and Wittenmark, Bj{\"o}rn},
  year={2013},
  publisher={Courier Corporation}
}

@book{landau2011adaptive,
  title={Adaptive control: algorithms, analysis and applications},
  author={Landau, Ioan Dor{\'e} and Lozano, Rogelio and M'Saad, Mohammed and Karimi, Alireza},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@book{tao2003adaptive,
  title={Adaptive control design and analysis},
  author={Tao, Gang},
  volume={37},
  year={2003},
  publisher={John Wiley \& Sons}
}

@book{goodwin2014adaptive,
  title={Adaptive filtering prediction and control},
  author={Goodwin, Graham C and Sin, Kwai Sang},
  year={2014},
  publisher={Courier Corporation}
}

@misc{sastry1990adaptive,
  title={Adaptive control: stability, convergence, and robustness},
  author={Sastry, Shankar and Bodson, Marc and Bartram, James F},
  year={1990},
  publisher={Acoustical Society of America}
}

@inproceedings{moskovitz2022towards,
  title={Towards an Understanding of Default Policies in Multitask Policy Optimization},
  author={Moskovitz, Ted and Arbel, Michael and Parker-Holder, Jack and Pacchiano, Aldo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10661--10686},
  year={2022},
  organization={PMLR}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@inproceedings{du2019provably,
  title={Provably efficient {RL} with rich observations via latent state decoding},
  author={Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={1665--1674},
  year={2019},
  organization={PMLR}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{gupta2022unpacking,
  title={Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity},
  author={Gupta, Abhishek and Pacchiano, Aldo and Zhai, Yuexiang and Kakade, Sham M and Levine, Sergey},
  journal={arXiv preprint arXiv:2210.09579},
  year={2022}
}

@article{liu2022revolver,
  title={REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer},
  author={Liu, Xingyu and Pathak, Deepak and Kitani, Kris M},
  journal={arXiv preprint arXiv:2202.05244},
  year={2022}
}

@article{bassich2020curriculum,
  title={Curriculum learning with a progression function},
  author={Bassich, Andrea and Foglino, Francesco and Leonetti, Matteo and Kudenko, Daniel},
  journal={arXiv preprint arXiv:2008.00511},
  year={2020}
}

@inproceedings{clavera2018model,
  title={Model-based reinforcement learning via meta-policy optimization},
  author={Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  booktitle={Conference on Robot Learning},
  pages={617--629},
  year={2018},
  organization={PMLR}
}

@article{agarwal2022provable,
  title={Provable Benefits of Representational Transfer in Reinforcement Learning},
  author={Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
  journal={arXiv preprint arXiv:2205.14571},
  year={2022}
}

@inproceedings{tian2019elf,
  title={Elf opengo: An analysis and open reimplementation of alphazero},
  author={Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, Larry},
  booktitle={International Conference on Machine Learning},
  pages={6244--6253},
  year={2019},
  organization={PMLR}
}