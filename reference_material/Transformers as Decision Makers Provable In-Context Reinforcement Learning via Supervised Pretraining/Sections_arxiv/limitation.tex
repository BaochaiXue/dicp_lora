
\section{Limitation and discussion}
\label{sec:limitation}
In this section, we discuss some limitations of our work and some potential future directions. 

\paragraph{Distribution ratio} In Theorem~\ref{thm:diff_reward}, our regret bound of the learned algorithms $\sAlg_{\esttfpar}$ depends on the distribution ratio $\distratio_{\sAlg_{\shortexp},\sAlg_0}$. While in cases like algorithm distillation~\citep{laskin2022context} the distribution ratio equals one since the offline algorithm matches the expert algorithm, in the worst case, the ratio can exponentially depend on $\totlen$ or even become arbitrarily large. To control the distribution ratio in practice, one approach is to augment the offline trajectory dataset with a portion of trajectories generated by an expert algorithm or no-regret algorithms resembling the expert algorithm. On the other hand, further research could investigate structural assumptions of decision-making problems that avoid pessimistic dependence on the distribution ratio in regret bounds. 

\paragraph{Guarantee of pretrained transformer} Our statistical result (Theorem~\ref{thm:diff_reward}) only guarantees that the pretrained transformer learns an ``algorithm'' matching the expert algorithm under the pre-training distribution, even though our approximation results (Theorem~\ref{thm:approx_smooth_linucb},~\ref{thm:approx_thompson_linear},~\ref{thm:approx_ucbvi}) show the existence of a transformer approximating the expert algorithm over the entire input space. In our early experiments, we noticed the learned transformers do not generalize well on out-of-distribution instances, such as with shifted reward distributions or increased number of runs $\totlen$. Similar phenomena occur in other in-context learning problems (e.g.~\cite{garg2022can}). Understanding the actual algorithm implemented by the pretrained transformer through theoretical and empirical analysis is an interesting question for future work.
%Hence, it is important to understand in the future, through both empirical and theoretical analysis, if pretrained transformers truly imitate expert algorithms or simply implement alternative algorithms that mimic expert behaviors on the specific problem distribution $\prior$. 


\paragraph{Alternative pretraining methods} Our theoretical results study pretraining the transformer by maximizing the log-likelihood of i.i.d. offline trajectories as in Eq.~(\ref{eq:general_mle}). This aligns with standard supervised pretraining of large language models. However, alternative pretraining methods may also be effective. For instance, one could replace the log-probability in Eq.~(\ref{eq:general_mle}) with an $\ell_2$ loss for continuous action spaces, consider other objectives like cumulative reward \citep{duan2016rl}, or explore goal-conditioned reinforcement learning \citep{chen2021decision} for in-context RL. While our work focuses on log-likelihood pretraining, theoretical investigation of alternative methods is an interesting direction for future work. 

\paragraph{Possibility of surpassing the expert algorithm by online training} Our work considers offline pretraining by imitating the expert algorithm (i.e., $\osAlg_\shortexp$), which can only learn a transformer matching the expert's performance at best. However, through online training, where the transformer interacts with the environment, the learned transformer may surpass existing experts by training to improve itself rather than imitating a specific algorithm. Investigating whether online training enables surpassing expert algorithms is an interesting direction for future work. 

% \paragraph{Internal behaviors of transformers} Our work demonstrates the ability of transformers to implement complicated RL algorithms efficiently, and establishes that via supervised pretraining an algorithm performs as well as the constructed RL algorithm on the problem distribution $\prior$ can be learned. However, the internal workings of transformers are still not well-understood. It is important to understand in the future, through both empirical and theoretical analysis, if transformers truly replicate expert algorithms or simply implement alternative algorithms that mimic expert behaviors on the specific problem distribution $\prior$. \red{ (do we want this part or not?) }




\paragraph{Implications for practice} While the focus of our work is primarily theoretical, our results lead to several practical implications for in-context reinforcement learning. One key implication is the importance of training labels (i.e., expert actions $\eaction$). When the expert algorithm depends solely on past observations, we can learn ${\sAlg}_{\shortexp}$ (see Theorem~\ref{thm:smooth_linucb}). In contrast, when ${\sAlg}_{\shortexp}$ is the optimal action $a^\star$ (involving knowledge of the underlying MDP), we can learn the posterior average of this algorithm given past observations. This corresponds to the Thompson sampling algorithm, as in Decision-Pretrained Transformers (see Theorem~\ref{thm:ts_linear_regret}). 

Furthermore, as discussed previously, the distribution ratio between the offline and expert algorithms may impact the generalization of the learned algorithm. Both our theory (see Theorem~\ref{thm:approx_smooth_linucb}) and simulations (see Figure~\ref{fig:compare_ratio}) show that a small distribution ratio between the offline algorithm $\sAlg_0$ and the expert algorithm $\osAlg_{\shortexp}$ is essential, otherwise the online performance of the learned algorithm may substantially degrade. This suggests that incorporating trajectories generated purely from the expert (``on-policy ICRL'') into the offline dataset is advantageous, when feasible. 

